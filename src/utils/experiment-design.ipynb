{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45fba2b-e75e-473b-8170-1a96d9d815bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca473b18-5b25-41ec-bcde-f686cefed816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>79.167145</td>\n",
       "      <td>80.182854</td>\n",
       "      <td>79.142860</td>\n",
       "      <td>80.145714</td>\n",
       "      <td>74.571281</td>\n",
       "      <td>55771100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>79.382858</td>\n",
       "      <td>79.575714</td>\n",
       "      <td>78.860001</td>\n",
       "      <td>79.018570</td>\n",
       "      <td>73.522530</td>\n",
       "      <td>58671200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>78.980003</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>77.204285</td>\n",
       "      <td>77.282860</td>\n",
       "      <td>71.907555</td>\n",
       "      <td>98116900</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>76.778572</td>\n",
       "      <td>78.114288</td>\n",
       "      <td>76.228569</td>\n",
       "      <td>77.704285</td>\n",
       "      <td>72.299644</td>\n",
       "      <td>103152700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>77.760002</td>\n",
       "      <td>77.994286</td>\n",
       "      <td>76.845711</td>\n",
       "      <td>77.148575</td>\n",
       "      <td>71.782608</td>\n",
       "      <td>79302300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>105.470001</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>104.889999</td>\n",
       "      <td>105.669998</td>\n",
       "      <td>102.653854</td>\n",
       "      <td>26133000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>2016-03-28</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.190002</td>\n",
       "      <td>105.059998</td>\n",
       "      <td>105.190002</td>\n",
       "      <td>102.187561</td>\n",
       "      <td>19411400</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>2016-03-29</td>\n",
       "      <td>104.889999</td>\n",
       "      <td>107.790001</td>\n",
       "      <td>104.879997</td>\n",
       "      <td>107.680000</td>\n",
       "      <td>104.606491</td>\n",
       "      <td>31190100</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>108.650002</td>\n",
       "      <td>110.419998</td>\n",
       "      <td>108.599998</td>\n",
       "      <td>109.559998</td>\n",
       "      <td>106.432831</td>\n",
       "      <td>45601100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>109.720001</td>\n",
       "      <td>109.900002</td>\n",
       "      <td>108.879997</td>\n",
       "      <td>108.989998</td>\n",
       "      <td>105.879097</td>\n",
       "      <td>25888400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>566 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2013-12-31   79.167145   80.182854   79.142860   80.145714   74.571281   \n",
       "1    2014-01-02   79.382858   79.575714   78.860001   79.018570   73.522530   \n",
       "2    2014-01-03   78.980003   79.099998   77.204285   77.282860   71.907555   \n",
       "3    2014-01-06   76.778572   78.114288   76.228569   77.704285   72.299644   \n",
       "4    2014-01-07   77.760002   77.994286   76.845711   77.148575   71.782608   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "561  2016-03-24  105.470001  106.250000  104.889999  105.669998  102.653854   \n",
       "562  2016-03-28  106.000000  106.190002  105.059998  105.190002  102.187561   \n",
       "563  2016-03-29  104.889999  107.790001  104.879997  107.680000  104.606491   \n",
       "564  2016-03-30  108.650002  110.419998  108.599998  109.559998  106.432831   \n",
       "565  2016-03-31  109.720001  109.900002  108.879997  108.989998  105.879097   \n",
       "\n",
       "        Volume  sentiment  \n",
       "0     55771100        0.0  \n",
       "1     58671200        1.0  \n",
       "2     98116900       -1.0  \n",
       "3    103152700        1.0  \n",
       "4     79302300        1.0  \n",
       "..         ...        ...  \n",
       "561   26133000        0.0  \n",
       "562   19411400       -1.0  \n",
       "563   31190100       -1.0  \n",
       "564   45601100        1.0  \n",
       "565   25888400        1.0  \n",
       "\n",
       "[566 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed_stock_data_aapl.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0282724c-98e6-47d6-a71c-19ad6ecebc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/49450801/ipykernel_1186529/1306136909.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_data.dropna(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Volume_Change</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>MA_5</th>\n",
       "      <th>MA_10</th>\n",
       "      <th>MA_20</th>\n",
       "      <th>MA_50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-03-13</th>\n",
       "      <td>70.956673</td>\n",
       "      <td>64435700</td>\n",
       "      <td>-0.011107</td>\n",
       "      <td>0.293069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.263152</td>\n",
       "      <td>71.044394</td>\n",
       "      <td>71.109581</td>\n",
       "      <td>71.029325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-14</th>\n",
       "      <td>70.159729</td>\n",
       "      <td>59299800</td>\n",
       "      <td>-0.011231</td>\n",
       "      <td>-0.079706</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.109377</td>\n",
       "      <td>71.023669</td>\n",
       "      <td>70.977604</td>\n",
       "      <td>70.941094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-17</th>\n",
       "      <td>70.433861</td>\n",
       "      <td>49886200</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>-0.158746</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>70.997594</td>\n",
       "      <td>71.010031</td>\n",
       "      <td>70.862274</td>\n",
       "      <td>70.879320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-18</th>\n",
       "      <td>71.056946</td>\n",
       "      <td>52411800</td>\n",
       "      <td>0.008846</td>\n",
       "      <td>0.050627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.872167</td>\n",
       "      <td>71.012169</td>\n",
       "      <td>70.764726</td>\n",
       "      <td>70.862308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-19</th>\n",
       "      <td>71.038239</td>\n",
       "      <td>56189000</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>0.072068</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>70.729090</td>\n",
       "      <td>70.997459</td>\n",
       "      <td>70.723875</td>\n",
       "      <td>70.837080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-24</th>\n",
       "      <td>102.653854</td>\n",
       "      <td>26133000</td>\n",
       "      <td>-0.004334</td>\n",
       "      <td>0.016710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.042441</td>\n",
       "      <td>102.146759</td>\n",
       "      <td>99.868692</td>\n",
       "      <td>95.960872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-28</th>\n",
       "      <td>102.187561</td>\n",
       "      <td>19411400</td>\n",
       "      <td>-0.004542</td>\n",
       "      <td>-0.257207</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>102.900607</td>\n",
       "      <td>102.431397</td>\n",
       "      <td>100.270875</td>\n",
       "      <td>96.122631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-29</th>\n",
       "      <td>104.606491</td>\n",
       "      <td>31190100</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.606793</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>103.244504</td>\n",
       "      <td>102.932668</td>\n",
       "      <td>100.804691</td>\n",
       "      <td>96.291609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-30</th>\n",
       "      <td>106.432831</td>\n",
       "      <td>45601100</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.462038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103.796294</td>\n",
       "      <td>103.416454</td>\n",
       "      <td>101.243304</td>\n",
       "      <td>96.543298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>105.879097</td>\n",
       "      <td>25888400</td>\n",
       "      <td>-0.005203</td>\n",
       "      <td>-0.432286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.351967</td>\n",
       "      <td>103.709833</td>\n",
       "      <td>101.643544</td>\n",
       "      <td>96.792995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj Close    Volume  Price_Change  Volume_Change  sentiment  \\\n",
       "Date                                                                       \n",
       "2014-03-13   70.956673  64435700     -0.011107       0.293069        0.0   \n",
       "2014-03-14   70.159729  59299800     -0.011231      -0.079706        1.0   \n",
       "2014-03-17   70.433861  49886200      0.003907      -0.158746       -1.0   \n",
       "2014-03-18   71.056946  52411800      0.008846       0.050627        1.0   \n",
       "2014-03-19   71.038239  56189000     -0.000263       0.072068       -1.0   \n",
       "...                ...       ...           ...            ...        ...   \n",
       "2016-03-24  102.653854  26133000     -0.004334       0.016710        0.0   \n",
       "2016-03-28  102.187561  19411400     -0.004542      -0.257207       -1.0   \n",
       "2016-03-29  104.606491  31190100      0.023671       0.606793       -1.0   \n",
       "2016-03-30  106.432831  45601100      0.017459       0.462038        1.0   \n",
       "2016-03-31  105.879097  25888400     -0.005203      -0.432286        1.0   \n",
       "\n",
       "                  MA_5       MA_10       MA_20      MA_50  \n",
       "Date                                                       \n",
       "2014-03-13   71.263152   71.044394   71.109581  71.029325  \n",
       "2014-03-14   71.109377   71.023669   70.977604  70.941094  \n",
       "2014-03-17   70.997594   71.010031   70.862274  70.879320  \n",
       "2014-03-18   70.872167   71.012169   70.764726  70.862308  \n",
       "2014-03-19   70.729090   70.997459   70.723875  70.837080  \n",
       "...                ...         ...         ...        ...  \n",
       "2016-03-24  103.042441  102.146759   99.868692  95.960872  \n",
       "2016-03-28  102.900607  102.431397  100.270875  96.122631  \n",
       "2016-03-29  103.244504  102.932668  100.804691  96.291609  \n",
       "2016-03-30  103.796294  103.416454  101.243304  96.543298  \n",
       "2016-03-31  104.351967  103.709833  101.643544  96.792995  \n",
       "\n",
       "[517 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare stock data\n",
    "def prepare_stock_data(df, ma_periods=[5, 10, 20, 50]):\n",
    "    data = df.copy()\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    for period in ma_periods:\n",
    "        data[f'MA_{period}'] = data['Adj Close'].rolling(window=period).mean()\n",
    "    data['Price_Change'] = data['Adj Close'].pct_change()\n",
    "    data['Volume_Change'] = data['Volume'].pct_change()\n",
    "    selected_features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'sentiment'] + \\\n",
    "                        [f'MA_{period}' for period in ma_periods]\n",
    "    processed_data = data[selected_features]\n",
    "    processed_data.dropna(inplace=True)\n",
    "    return processed_data\n",
    "\n",
    "df = prepare_stock_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975ea01f-8aba-43b3-829d-ec79902c2dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length), :])\n",
    "        y.append(data[i + sequence_length, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba7245c9-0892-40d1-a7cf-a923941a4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "953b615d-682a-4208-8783-5fedd8960493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.3):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size // 2,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size // 4,\n",
    "                           batch_first=True)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size // 4)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(hidden_size // 4, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First LSTM layer\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.bn1(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.bn2(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third LSTM layer\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.bn3(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Take the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdbfa13a-891c-4f15-be2a-be6b3363d1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in the model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def analyze_model_complexity(model, input_shape):\n",
    "    \"\"\"Analyze model complexity and architecture\"\"\"\n",
    "    # Create dummy input for visualization\n",
    "    batch_size = 32\n",
    "    dummy_input = torch.zeros(batch_size, *input_shape)\n",
    "    \n",
    "    # Get model summary\n",
    "    model_summary = summary(model, input_size=(batch_size, *input_shape), \n",
    "                          verbose=2, col_names=['input_size', 'output_size', \n",
    "                                              'num_params', 'kernel_size', \n",
    "                                              'mult_adds'])\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = count_parameters(model)\n",
    "    \n",
    "    # Calculate model complexity metrics\n",
    "    complexity_metrics = {\n",
    "        'Total Parameters': total_params,\n",
    "        'LSTM Layers': 3,\n",
    "        'Hidden Units': [256, 128, 32],  # Bidirectional doubles first two\n",
    "        'Dropout Rates': [0.3, 0.3, 0.2],\n",
    "        'Dense Layers': 2\n",
    "    }\n",
    "    \n",
    "    return complexity_metrics, model_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67f9f74d-2143-4728-8217-7a3251380e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += criterion(y_pred, y_batch).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ef7887-a22b-4217-974f-9da6a1403a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_and_analyze_model(df, features, sequence_length=20, test_size=0.2):\n",
    "    # Prepare data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = StockDataset(X_train, y_train)\n",
    "    test_dataset = StockDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = len(features)\n",
    "    model = StockLSTM(input_size=input_size)\n",
    "    \n",
    "    # Analyze model complexity\n",
    "    complexity_metrics, model_summary = analyze_model_complexity(model, (sequence_length, input_size))\n",
    "    \n",
    "    print(\"\\nModel Complexity Analysis:\")\n",
    "    for metric, value in complexity_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    \n",
    "    print(\"\\nDetailed Layer Analysis:\")\n",
    "    print(model_summary)\n",
    "    \n",
    "    return model, train_loader, test_loader, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25109816-58ba-4190-9312-cf90ce2d0753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "=====================================================================================================================================================================\n",
      "StockLSTM                                [32, 20, 8]               [32, 1]                   --                        --                        --\n",
      "â”œâ”€LSTM: 1-1                              [32, 20, 8]               [32, 20, 256]             141,312                   --                        90,439,680\n",
      "â”‚    â””â”€weight_ih_l0                                                                          â”œâ”€4,096                   [512, 8]\n",
      "â”‚    â””â”€weight_hh_l0                                                                          â”œâ”€65,536                  [512, 128]\n",
      "â”‚    â””â”€bias_ih_l0                                                                            â”œâ”€512                     [512]\n",
      "â”‚    â””â”€bias_hh_l0                                                                            â”œâ”€512                     [512]\n",
      "â”‚    â””â”€weight_ih_l0_reverse                                                                  â”œâ”€4,096                   [512, 8]\n",
      "â”‚    â””â”€weight_hh_l0_reverse                                                                  â”œâ”€65,536                  [512, 128]\n",
      "â”‚    â””â”€bias_ih_l0_reverse                                                                    â”œâ”€512                     [512]\n",
      "â”‚    â””â”€bias_hh_l0_reverse                                                                    â””â”€512                     [512]\n",
      "â”œâ”€BatchNorm1d: 1-2                       [32, 256, 20]             [32, 256, 20]             512                       --                        16,384\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€256                     [256]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€256                     [256]\n",
      "â”œâ”€Dropout: 1-3                           [32, 20, 256]             [32, 20, 256]             --                        --                        --\n",
      "â”œâ”€LSTM: 1-4                              [32, 20, 256]             [32, 20, 128]             164,864                   --                        105,512,960\n",
      "â”‚    â””â”€weight_ih_l0                                                                          â”œâ”€65,536                  [256, 256]\n",
      "â”‚    â””â”€weight_hh_l0                                                                          â”œâ”€16,384                  [256, 64]\n",
      "â”‚    â””â”€bias_ih_l0                                                                            â”œâ”€256                     [256]\n",
      "â”‚    â””â”€bias_hh_l0                                                                            â”œâ”€256                     [256]\n",
      "â”‚    â””â”€weight_ih_l0_reverse                                                                  â”œâ”€65,536                  [256, 256]\n",
      "â”‚    â””â”€weight_hh_l0_reverse                                                                  â”œâ”€16,384                  [256, 64]\n",
      "â”‚    â””â”€bias_ih_l0_reverse                                                                    â”œâ”€256                     [256]\n",
      "â”‚    â””â”€bias_hh_l0_reverse                                                                    â””â”€256                     [256]\n",
      "â”œâ”€BatchNorm1d: 1-5                       [32, 128, 20]             [32, 128, 20]             256                       --                        8,192\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€128                     [128]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€128                     [128]\n",
      "â”œâ”€Dropout: 1-6                           [32, 20, 128]             [32, 20, 128]             --                        --                        --\n",
      "â”œâ”€LSTM: 1-7                              [32, 20, 128]             [32, 20, 32]              20,736                    --                        13,271,040\n",
      "â”‚    â””â”€weight_ih_l0                                                                          â”œâ”€16,384                  [128, 128]\n",
      "â”‚    â””â”€weight_hh_l0                                                                          â”œâ”€4,096                   [128, 32]\n",
      "â”‚    â””â”€bias_ih_l0                                                                            â”œâ”€128                     [128]\n",
      "â”‚    â””â”€bias_hh_l0                                                                            â””â”€128                     [128]\n",
      "â”œâ”€BatchNorm1d: 1-8                       [32, 32, 20]              [32, 32, 20]              64                        --                        2,048\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€32                      [32]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€32                      [32]\n",
      "â”œâ”€Dropout: 1-9                           [32, 20, 32]              [32, 20, 32]              --                        --                        --\n",
      "â”œâ”€Linear: 1-10                           [32, 32]                  [32, 16]                  528                       --                        16,896\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€512                     [32, 16]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€16                      [16]\n",
      "â”œâ”€ReLU: 1-11                             [32, 16]                  [32, 16]                  --                        --                        --\n",
      "â”œâ”€Linear: 1-12                           [32, 16]                  [32, 1]                   17                        --                        544\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€16                      [16, 1]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€1                       [1]\n",
      "=====================================================================================================================================================================\n",
      "Total params: 328,289\n",
      "Trainable params: 328,289\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 209.27\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 4.26\n",
      "Params size (MB): 1.31\n",
      "Estimated Total Size (MB): 5.60\n",
      "=====================================================================================================================================================================\n",
      "\n",
      "Model Complexity Analysis:\n",
      "Total Parameters: 328289\n",
      "LSTM Layers: 3\n",
      "Hidden Units: [256, 128, 32]\n",
      "Dropout Rates: [0.3, 0.3, 0.2]\n",
      "Dense Layers: 2\n",
      "\n",
      "Detailed Layer Analysis:\n",
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "=====================================================================================================================================================================\n",
      "StockLSTM                                [32, 20, 8]               [32, 1]                   --                        --                        --\n",
      "â”œâ”€LSTM: 1-1                              [32, 20, 8]               [32, 20, 256]             141,312                   --                        90,439,680\n",
      "â”‚    â””â”€weight_ih_l0                                                                          â”œâ”€4,096                   [512, 8]\n",
      "â”‚    â””â”€weight_hh_l0                                                                          â”œâ”€65,536                  [512, 128]\n",
      "â”‚    â””â”€bias_ih_l0                                                                            â”œâ”€512                     [512]\n",
      "â”‚    â””â”€bias_hh_l0                                                                            â”œâ”€512                     [512]\n",
      "â”‚    â””â”€weight_ih_l0_reverse                                                                  â”œâ”€4,096                   [512, 8]\n",
      "â”‚    â””â”€weight_hh_l0_reverse                                                                  â”œâ”€65,536                  [512, 128]\n",
      "â”‚    â””â”€bias_ih_l0_reverse                                                                    â”œâ”€512                     [512]\n",
      "â”‚    â””â”€bias_hh_l0_reverse                                                                    â””â”€512                     [512]\n",
      "â”œâ”€BatchNorm1d: 1-2                       [32, 256, 20]             [32, 256, 20]             512                       --                        16,384\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€256                     [256]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€256                     [256]\n",
      "â”œâ”€Dropout: 1-3                           [32, 20, 256]             [32, 20, 256]             --                        --                        --\n",
      "â”œâ”€LSTM: 1-4                              [32, 20, 256]             [32, 20, 128]             164,864                   --                        105,512,960\n",
      "â”‚    â””â”€weight_ih_l0                                                                          â”œâ”€65,536                  [256, 256]\n",
      "â”‚    â””â”€weight_hh_l0                                                                          â”œâ”€16,384                  [256, 64]\n",
      "â”‚    â””â”€bias_ih_l0                                                                            â”œâ”€256                     [256]\n",
      "â”‚    â””â”€bias_hh_l0                                                                            â”œâ”€256                     [256]\n",
      "â”‚    â””â”€weight_ih_l0_reverse                                                                  â”œâ”€65,536                  [256, 256]\n",
      "â”‚    â””â”€weight_hh_l0_reverse                                                                  â”œâ”€16,384                  [256, 64]\n",
      "â”‚    â””â”€bias_ih_l0_reverse                                                                    â”œâ”€256                     [256]\n",
      "â”‚    â””â”€bias_hh_l0_reverse                                                                    â””â”€256                     [256]\n",
      "â”œâ”€BatchNorm1d: 1-5                       [32, 128, 20]             [32, 128, 20]             256                       --                        8,192\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€128                     [128]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€128                     [128]\n",
      "â”œâ”€Dropout: 1-6                           [32, 20, 128]             [32, 20, 128]             --                        --                        --\n",
      "â”œâ”€LSTM: 1-7                              [32, 20, 128]             [32, 20, 32]              20,736                    --                        13,271,040\n",
      "â”‚    â””â”€weight_ih_l0                                                                          â”œâ”€16,384                  [128, 128]\n",
      "â”‚    â””â”€weight_hh_l0                                                                          â”œâ”€4,096                   [128, 32]\n",
      "â”‚    â””â”€bias_ih_l0                                                                            â”œâ”€128                     [128]\n",
      "â”‚    â””â”€bias_hh_l0                                                                            â””â”€128                     [128]\n",
      "â”œâ”€BatchNorm1d: 1-8                       [32, 32, 20]              [32, 32, 20]              64                        --                        2,048\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€32                      [32]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€32                      [32]\n",
      "â”œâ”€Dropout: 1-9                           [32, 20, 32]              [32, 20, 32]              --                        --                        --\n",
      "â”œâ”€Linear: 1-10                           [32, 32]                  [32, 16]                  528                       --                        16,896\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€512                     [32, 16]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€16                      [16]\n",
      "â”œâ”€ReLU: 1-11                             [32, 16]                  [32, 16]                  --                        --                        --\n",
      "â”œâ”€Linear: 1-12                           [32, 16]                  [32, 1]                   17                        --                        544\n",
      "â”‚    â””â”€weight                                                                                â”œâ”€16                      [16, 1]\n",
      "â”‚    â””â”€bias                                                                                  â””â”€1                       [1]\n",
      "=====================================================================================================================================================================\n",
      "Total params: 328,289\n",
      "Trainable params: 328,289\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 209.27\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 4.26\n",
      "Params size (MB): 1.31\n",
      "Estimated Total Size (MB): 5.60\n",
      "=====================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50']\n",
    "model, train_loader, test_loader, scaler = prepare_and_analyze_model(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636cebd-c66d-4d78-b9e0-9b2cca80e774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
