{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74208f7e-6874-4c26-bfe0-581773405fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Using Pytorch Only #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2c9aba-0efe-49e2-b3c7-79e3a84bbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd58e9af-686e-4575-bea2-161c64693052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed_stock_data_goog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38fb959-ffe2-4afd-b40d-db68f6860ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/49569950/ipykernel_1188617/2596264522.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_data.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Prepare stock data\n",
    "def prepare_stock_data(df, ma_periods=[5, 10, 20, 50]):\n",
    "    data = df.copy()\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    for period in ma_periods:\n",
    "        data[f'MA_{period}'] = data['Adj Close'].rolling(window=period).mean()\n",
    "    data['Price_Change'] = data['Adj Close'].pct_change()\n",
    "    data['Volume_Change'] = data['Volume'].pct_change()\n",
    "    selected_features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'sentiment'] + \\\n",
    "                        [f'MA_{period}' for period in ma_periods]\n",
    "    processed_data = data[selected_features]\n",
    "    processed_data.dropna(inplace=True)\n",
    "    return processed_data\n",
    "\n",
    "df = prepare_stock_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bcf26a1-ad32-4706-8784-eebe76ccb3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim * 2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim * 2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.dropout(self.relu(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db23a014-bfb3-451a-948b-8aed7b9684b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length), :])\n",
    "        y.append(data[i + sequence_length, 0])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd372dda-9a82-40b4-ba22-ff215fd08d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare and train model \n",
    "def prepare_and_train_model(df, features, sequence_length=20, test_size=0.2, learning_rate=0.001, epochs=100):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    X, y = create_sequences(scaled_data, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    # Model, loss, optimizer\n",
    "    input_dim = X_train.shape[2]\n",
    "    model = LSTMModel(input_dim=input_dim, hidden_dim=128, layer_dim=2, output_dim=1)\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output.view(-1), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_epoch_loss:.4f}')\n",
    "    \n",
    "    return model, scaler, (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2c886e-8fc6-4fad-afd4-ccfcf3d3ab36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "def evaluate_stock_predictions(y_true, y_pred):\n",
    "    y_true, y_pred = y_true.detach().numpy(), y_pred.detach().numpy()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    results = {\n",
    "        'Mean Squared Error (MSE)': mse,\n",
    "        'Root Mean Squared Error (RMSE)': rmse,\n",
    "        'Mean Absolute Error (MAE)': mae,\n",
    "        'Mean Absolute Percentage Error (MAPE)': mape,\n",
    "        'R-squared (RÂ²)': r2\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee03c3-2c5e-4ea7-af36-3e0bea259e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, stock, path='stock_model.pth'):\n",
    "    \"\"\"Save the model and scaler\"\"\"\n",
    "    path \n",
    "    model_state = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    torch.save(model_state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61bfe294-f8db-43fb-956f-98045641dc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0245\n",
      "Epoch [2/100], Loss: 0.0110\n",
      "Epoch [3/100], Loss: 0.0081\n",
      "Epoch [4/100], Loss: 0.0058\n",
      "Epoch [5/100], Loss: 0.0057\n",
      "Epoch [6/100], Loss: 0.0049\n",
      "Epoch [7/100], Loss: 0.0047\n",
      "Epoch [8/100], Loss: 0.0038\n",
      "Epoch [9/100], Loss: 0.0036\n",
      "Epoch [10/100], Loss: 0.0037\n",
      "Epoch [11/100], Loss: 0.0034\n",
      "Epoch [12/100], Loss: 0.0040\n",
      "Epoch [13/100], Loss: 0.0038\n",
      "Epoch [14/100], Loss: 0.0034\n",
      "Epoch [15/100], Loss: 0.0036\n",
      "Epoch [16/100], Loss: 0.0028\n",
      "Epoch [17/100], Loss: 0.0029\n",
      "Epoch [18/100], Loss: 0.0025\n",
      "Epoch [19/100], Loss: 0.0021\n",
      "Epoch [20/100], Loss: 0.0024\n",
      "Epoch [21/100], Loss: 0.0027\n",
      "Epoch [22/100], Loss: 0.0025\n",
      "Epoch [23/100], Loss: 0.0027\n",
      "Epoch [24/100], Loss: 0.0033\n",
      "Epoch [25/100], Loss: 0.0030\n",
      "Epoch [26/100], Loss: 0.0026\n",
      "Epoch [27/100], Loss: 0.0023\n",
      "Epoch [28/100], Loss: 0.0025\n",
      "Epoch [29/100], Loss: 0.0020\n",
      "Epoch [30/100], Loss: 0.0026\n",
      "Epoch [31/100], Loss: 0.0024\n",
      "Epoch [32/100], Loss: 0.0021\n",
      "Epoch [33/100], Loss: 0.0024\n",
      "Epoch [34/100], Loss: 0.0021\n",
      "Epoch [35/100], Loss: 0.0019\n",
      "Epoch [36/100], Loss: 0.0020\n",
      "Epoch [37/100], Loss: 0.0024\n",
      "Epoch [38/100], Loss: 0.0018\n",
      "Epoch [39/100], Loss: 0.0031\n",
      "Epoch [40/100], Loss: 0.0018\n",
      "Epoch [41/100], Loss: 0.0020\n",
      "Epoch [42/100], Loss: 0.0016\n",
      "Epoch [43/100], Loss: 0.0017\n",
      "Epoch [44/100], Loss: 0.0018\n",
      "Epoch [45/100], Loss: 0.0019\n",
      "Epoch [46/100], Loss: 0.0018\n",
      "Epoch [47/100], Loss: 0.0024\n",
      "Epoch [48/100], Loss: 0.0021\n",
      "Epoch [49/100], Loss: 0.0016\n",
      "Epoch [50/100], Loss: 0.0020\n",
      "Epoch [51/100], Loss: 0.0017\n",
      "Epoch [52/100], Loss: 0.0019\n",
      "Epoch [53/100], Loss: 0.0014\n",
      "Epoch [54/100], Loss: 0.0015\n",
      "Epoch [55/100], Loss: 0.0018\n",
      "Epoch [56/100], Loss: 0.0015\n",
      "Epoch [57/100], Loss: 0.0016\n",
      "Epoch [58/100], Loss: 0.0018\n",
      "Epoch [59/100], Loss: 0.0016\n",
      "Epoch [60/100], Loss: 0.0018\n",
      "Epoch [61/100], Loss: 0.0021\n",
      "Epoch [62/100], Loss: 0.0020\n",
      "Epoch [63/100], Loss: 0.0014\n",
      "Epoch [64/100], Loss: 0.0014\n",
      "Epoch [65/100], Loss: 0.0016\n",
      "Epoch [66/100], Loss: 0.0014\n",
      "Epoch [67/100], Loss: 0.0014\n",
      "Epoch [68/100], Loss: 0.0015\n",
      "Epoch [69/100], Loss: 0.0020\n",
      "Epoch [70/100], Loss: 0.0019\n",
      "Epoch [71/100], Loss: 0.0015\n",
      "Epoch [72/100], Loss: 0.0013\n",
      "Epoch [73/100], Loss: 0.0015\n",
      "Epoch [74/100], Loss: 0.0012\n",
      "Epoch [75/100], Loss: 0.0019\n",
      "Epoch [76/100], Loss: 0.0017\n",
      "Epoch [77/100], Loss: 0.0017\n",
      "Epoch [78/100], Loss: 0.0016\n",
      "Epoch [79/100], Loss: 0.0015\n",
      "Epoch [80/100], Loss: 0.0013\n",
      "Epoch [81/100], Loss: 0.0014\n",
      "Epoch [82/100], Loss: 0.0012\n",
      "Epoch [83/100], Loss: 0.0012\n",
      "Epoch [84/100], Loss: 0.0014\n",
      "Epoch [85/100], Loss: 0.0014\n",
      "Epoch [86/100], Loss: 0.0015\n",
      "Epoch [87/100], Loss: 0.0015\n",
      "Epoch [88/100], Loss: 0.0016\n",
      "Epoch [89/100], Loss: 0.0015\n",
      "Epoch [90/100], Loss: 0.0012\n",
      "Epoch [91/100], Loss: 0.0012\n",
      "Epoch [92/100], Loss: 0.0014\n",
      "Epoch [93/100], Loss: 0.0014\n",
      "Epoch [94/100], Loss: 0.0014\n",
      "Epoch [95/100], Loss: 0.0013\n",
      "Epoch [96/100], Loss: 0.0012\n",
      "Epoch [97/100], Loss: 0.0013\n",
      "Epoch [98/100], Loss: 0.0016\n",
      "Epoch [99/100], Loss: 0.0016\n",
      "Epoch [100/100], Loss: 0.0013\n",
      "Mean Squared Error (MSE): 0.0031\n",
      "Root Mean Squared Error (RMSE): 0.0558\n",
      "Mean Absolute Error (MAE): 0.0424\n",
      "Mean Absolute Percentage Error (MAPE): 5.2691\n",
      "R-squared (RÂ²): 0.5591\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'sentiment']\n",
    "model, scaler, (X_train, X_test, y_train, y_test) = prepare_and_train_model(df, features, sequence_length=24)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "model.eval()\n",
    "predictions = model(X_test).view(-1).detach()\n",
    "results = evaluate_stock_predictions(y_test, predictions)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c868f0d-9035-4870-ad6f-c3a7dc9d428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5acf13-a624-4e81-99bb-e525b2edc006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0932\n",
      "Epoch [2/100], Loss: 0.0214\n",
      "Epoch [3/100], Loss: 0.0142\n",
      "Epoch [4/100], Loss: 0.0094\n",
      "Epoch [5/100], Loss: 0.0113\n",
      "Epoch [6/100], Loss: 0.0085\n",
      "Epoch [7/100], Loss: 0.0092\n",
      "Epoch [8/100], Loss: 0.0087\n",
      "Epoch [9/100], Loss: 0.0084\n",
      "Epoch [10/100], Loss: 0.0090\n",
      "Epoch [11/100], Loss: 0.0091\n",
      "Epoch [12/100], Loss: 0.0080\n",
      "Epoch [13/100], Loss: 0.0065\n",
      "Epoch [14/100], Loss: 0.0082\n",
      "Epoch [15/100], Loss: 0.0065\n",
      "Epoch [16/100], Loss: 0.0065\n",
      "Epoch [17/100], Loss: 0.0083\n",
      "Epoch [18/100], Loss: 0.0073\n",
      "Epoch [19/100], Loss: 0.0065\n",
      "Epoch [20/100], Loss: 0.0080\n",
      "Epoch [21/100], Loss: 0.0077\n",
      "Epoch [22/100], Loss: 0.0065\n",
      "Epoch [23/100], Loss: 0.0063\n",
      "Epoch [24/100], Loss: 0.0083\n",
      "Epoch [25/100], Loss: 0.0067\n",
      "Epoch [26/100], Loss: 0.0068\n",
      "Epoch [27/100], Loss: 0.0062\n",
      "Epoch [28/100], Loss: 0.0067\n",
      "Epoch [29/100], Loss: 0.0063\n",
      "Epoch [30/100], Loss: 0.0066\n",
      "Epoch [31/100], Loss: 0.0054\n",
      "Epoch [32/100], Loss: 0.0065\n",
      "Epoch [33/100], Loss: 0.0065\n",
      "Epoch [34/100], Loss: 0.0066\n",
      "Epoch [35/100], Loss: 0.0061\n",
      "Epoch [36/100], Loss: 0.0070\n",
      "Epoch [37/100], Loss: 0.0058\n",
      "Epoch [38/100], Loss: 0.0062\n",
      "Epoch [39/100], Loss: 0.0064\n",
      "Epoch [40/100], Loss: 0.0052\n",
      "Epoch [41/100], Loss: 0.0059\n",
      "Epoch [42/100], Loss: 0.0058\n",
      "Epoch [43/100], Loss: 0.0073\n",
      "Epoch [44/100], Loss: 0.0059\n",
      "Epoch [45/100], Loss: 0.0058\n",
      "Epoch [46/100], Loss: 0.0050\n",
      "Epoch [47/100], Loss: 0.0058\n",
      "Epoch [48/100], Loss: 0.0050\n",
      "Epoch [49/100], Loss: 0.0045\n",
      "Epoch [50/100], Loss: 0.0062\n",
      "Epoch [51/100], Loss: 0.0054\n",
      "Epoch [52/100], Loss: 0.0053\n",
      "Epoch [53/100], Loss: 0.0043\n",
      "Epoch [54/100], Loss: 0.0047\n",
      "Epoch [55/100], Loss: 0.0052\n",
      "Epoch [56/100], Loss: 0.0052\n",
      "Epoch [57/100], Loss: 0.0051\n",
      "Epoch [58/100], Loss: 0.0046\n",
      "Epoch [59/100], Loss: 0.0048\n",
      "Epoch [60/100], Loss: 0.0048\n",
      "Epoch [61/100], Loss: 0.0044\n",
      "Epoch [62/100], Loss: 0.0047\n",
      "Epoch [63/100], Loss: 0.0045\n",
      "Epoch [64/100], Loss: 0.0042\n",
      "Epoch [65/100], Loss: 0.0039\n",
      "Epoch [66/100], Loss: 0.0041\n",
      "Epoch [67/100], Loss: 0.0039\n",
      "Epoch [68/100], Loss: 0.0046\n",
      "Epoch [69/100], Loss: 0.0039\n",
      "Epoch [70/100], Loss: 0.0039\n",
      "Epoch [71/100], Loss: 0.0040\n",
      "Epoch [72/100], Loss: 0.0044\n",
      "Epoch [73/100], Loss: 0.0037\n",
      "Epoch [74/100], Loss: 0.0034\n",
      "Epoch [75/100], Loss: 0.0038\n",
      "Epoch [76/100], Loss: 0.0038\n",
      "Epoch [77/100], Loss: 0.0033\n",
      "Epoch [78/100], Loss: 0.0038\n",
      "Epoch [79/100], Loss: 0.0034\n",
      "Epoch [80/100], Loss: 0.0038\n",
      "Epoch [81/100], Loss: 0.0036\n",
      "Epoch [82/100], Loss: 0.0041\n",
      "Epoch [83/100], Loss: 0.0031\n",
      "Epoch [84/100], Loss: 0.0040\n",
      "Epoch [85/100], Loss: 0.0029\n",
      "Epoch [86/100], Loss: 0.0032\n",
      "Epoch [87/100], Loss: 0.0036\n",
      "Epoch [88/100], Loss: 0.0035\n",
      "Epoch [89/100], Loss: 0.0037\n",
      "Epoch [90/100], Loss: 0.0033\n",
      "Epoch [91/100], Loss: 0.0029\n",
      "Epoch [92/100], Loss: 0.0035\n",
      "Epoch [93/100], Loss: 0.0030\n",
      "Epoch [94/100], Loss: 0.0031\n",
      "Epoch [95/100], Loss: 0.0034\n",
      "Epoch [96/100], Loss: 0.0029\n",
      "Epoch [97/100], Loss: 0.0037\n",
      "Epoch [98/100], Loss: 0.0028\n",
      "Epoch [99/100], Loss: 0.0032\n",
      "Epoch [100/100], Loss: 0.0031\n",
      "Mean Squared Error (MSE): 0.0026\n",
      "Root Mean Squared Error (RMSE): 0.0512\n",
      "Mean Absolute Error (MAE): 0.0428\n",
      "Mean Absolute Percentage Error (MAPE): 8.1873\n",
      "R-squared (RÂ²): 0.8609\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50']\n",
    "model, scaler, (X_train, X_test, y_train, y_test) = prepare_and_train_model(df, features, sequence_length=24)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "model.eval()\n",
    "predictions = model(X_test).view(-1).detach()\n",
    "results = evaluate_stock_predictions(y_test, predictions)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c868e2e-a8c9-4620-beab-a451ef2dccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17c7ed-509f-4b9b-ab1b-40a24c8a8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Feature Importance ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea16c92d-5f76-4ea8-923d-11e1da110e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_permutation_importance(model, X, y, features, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Calculate permutation importance for LSTM model features\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained LSTM model\n",
    "    X: input tensor\n",
    "    y: target tensor\n",
    "    features: list of feature names\n",
    "    n_repeats: number of times to repeat permutation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    baseline_pred = model(X).view(-1)\n",
    "    baseline_loss = torch.nn.MSELoss()(baseline_pred, y)\n",
    "    \n",
    "    importance_scores = []\n",
    "    \n",
    "    for feature_idx in range(X.shape[2]):  # Loop through each feature\n",
    "        feature_importance = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X.clone()\n",
    "            # Permute the feature across all sequences\n",
    "            permuted_values = X_permuted[:, :, feature_idx][torch.randperm(X.shape[0])]\n",
    "            X_permuted[:, :, feature_idx] = permuted_values\n",
    "            \n",
    "            # Calculate new loss\n",
    "            with torch.no_grad():\n",
    "                new_pred = model(X_permuted).view(-1)\n",
    "                new_loss = torch.nn.MSELoss()(new_pred, y)\n",
    "            \n",
    "            # Importance is increase in loss\n",
    "            importance = (new_loss - baseline_loss).item()\n",
    "            feature_importance.append(importance)\n",
    "        \n",
    "        importance_scores.append(np.mean(feature_importance))\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importance_scores\n",
    "    })\n",
    "    return importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "def calculate_integrated_gradients(model, X, features, n_steps=50):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using integrated gradients\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained LSTM model\n",
    "    X: input tensor\n",
    "    features: list of feature names\n",
    "    n_steps: number of steps for path integral\n",
    "    \"\"\"\n",
    "    ig = IntegratedGradients(model)\n",
    "    baseline = torch.zeros_like(X)\n",
    "    \n",
    "    # Calculate attributions\n",
    "    attributions = ig.attribute(X, baseline, n_steps=n_steps)\n",
    "    \n",
    "    # Average attributions across sequences and samples\n",
    "    feature_importance = torch.mean(torch.abs(attributions), dim=(0, 1)).detach().numpy()\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "    return importance_df.sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9594c52-56ca-423a-9b8d-010e786774dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance_df, title):\n",
    "    \"\"\"\n",
    "    Plot feature importance results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def analyze_feature_importance(model, X_test, y_test, features):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained LSTM model\n",
    "    X_test: test input tensor\n",
    "    y_test: test target tensor\n",
    "    features: list of feature names\n",
    "    \"\"\"\n",
    "    # Calculate permutation importance\n",
    "    perm_importance = calculate_permutation_importance(model, X_test, y_test, features)\n",
    "    \n",
    "    # Calculate integrated gradients importance\n",
    "    ig_importance = calculate_integrated_gradients(model, X_test, features)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(perm_importance['feature'], perm_importance['importance'])\n",
    "    plt.title('Permutation Importance')\n",
    "    plt.xlabel('Increase in Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(ig_importance['feature'], ig_importance['importance'])\n",
    "    plt.title('Integrated Gradients Importance')\n",
    "    plt.xlabel('Attribution Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return {\n",
    "        'permutation': perm_importance,\n",
    "        'integrated_gradients': ig_importance\n",
    "    }\n",
    "\n",
    "def analyze_model_features(model, X_test, y_test, features):\n",
    "    \"\"\"\n",
    "    Wrapper function to analyze feature importance for your LSTM model\n",
    "    \"\"\"\n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    importance_results = analyze_feature_importance(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        features=features\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPermutation Importance:\")\n",
    "    print(importance_results['permutation'])\n",
    "    print(\"\\nIntegrated Gradients Importance:\")\n",
    "    print(importance_results['integrated_gradients'])\n",
    "    \n",
    "    return importance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eebe5986-1e3e-4da1-88f0-b8f4fc3e70e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation Importance:\n",
      "         feature  importance\n",
      "0      Adj Close    0.003691\n",
      "2   Price_Change    0.001197\n",
      "4           MA_5    0.000898\n",
      "3  Volume_Change    0.000013\n",
      "8      sentiment    0.000010\n",
      "6          MA_20   -0.000007\n",
      "5          MA_10   -0.000012\n",
      "1         Volume   -0.000089\n",
      "7          MA_50   -0.000207\n",
      "\n",
      "Integrated Gradients Importance:\n",
      "         feature  importance\n",
      "0      Adj Close    0.023612\n",
      "4           MA_5    0.013212\n",
      "5          MA_10    0.005543\n",
      "2   Price_Change    0.005395\n",
      "6          MA_20    0.002479\n",
      "7          MA_50    0.001450\n",
      "8      sentiment    0.000905\n",
      "1         Volume    0.000734\n",
      "3  Volume_Change    0.000136\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "importance_results = analyze_model_features(model, X_test, y_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648bc04-e17c-453e-ae3a-27b4667778e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd43b4-e297-44ed-aadb-6c9415cd2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Walk forward Cross Validation ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504bf9d6-b9e7-4590-a37c-a8c5937d06dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_and_train_model(df, features, sequence_length=20, n_splits=5, learning_rate=0.001, epochs=100):\n",
    "    \"\"\"\n",
    "    Prepare and train the model using walk-forward optimization with TimeSeriesSplit\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    X, y = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    # Lists to store performance metrics across folds\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Initialize best model tracking\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Walk-forward cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "        \n",
    "        # Initialize model, criterion, and optimizer\n",
    "        input_dim = X_train.shape[2]\n",
    "        model = LSTMModel(input_dim=input_dim, hidden_dim=128, layer_dim=2, output_dim=1)\n",
    "        criterion = nn.HuberLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Training loop for this fold\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_X)\n",
    "                loss = criterion(output.view(-1), batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_actuals = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    output = model(batch_X)\n",
    "                    val_loss += criterion(output.view(-1), batch_y).item()\n",
    "                    val_predictions.extend(output.view(-1).numpy())\n",
    "                    val_actuals.extend(batch_y.numpy())\n",
    "            \n",
    "            # Calculate average losses\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "            # Track best model based on validation loss\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict().copy()\n",
    "        \n",
    "        # Calculate fold metrics\n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_actuals = np.array(val_actuals)\n",
    "        fold_results = {\n",
    "            'fold': fold + 1,\n",
    "            'mse': mean_squared_error(val_actuals, val_predictions),\n",
    "            'rmse': np.sqrt(mean_squared_error(val_actuals, val_predictions)),\n",
    "            'mae': mean_absolute_error(val_actuals, val_predictions),\n",
    "            'r2': r2_score(val_actuals, val_predictions),\n",
    "            'mape': np.mean(np.abs((val_actuals - val_predictions) / val_actuals)) * 100\n",
    "        }\n",
    "        fold_metrics.append(fold_results)\n",
    "        \n",
    "        print(\"\\nFold Results:\")\n",
    "        for metric, value in fold_results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Create final model with best parameters\n",
    "    final_model = LSTMModel(input_dim=input_dim, hidden_dim=128, layer_dim=2, output_dim=1)\n",
    "    final_model.load_state_dict(best_model)\n",
    "    \n",
    "    # Calculate and print average metrics across folds\n",
    "    print(\"\\nAverage Metrics Across All Folds:\")\n",
    "    avg_metrics = {}\n",
    "    metric_keys = ['mse', 'rmse', 'mae', 'r2', 'mape']\n",
    "    for metric in metric_keys:\n",
    "        avg_metrics[metric] = np.mean([fold[metric] for fold in fold_metrics])\n",
    "        print(f\"Average {metric.upper()}: {avg_metrics[metric]:.4f}\")\n",
    "    \n",
    "    return final_model, scaler, fold_metrics, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f21e09-c245-46ff-a7e0-89b75925b14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Epoch [1/100], Train Loss: 0.0107, Val Loss: 0.0615\n",
      "Epoch [2/100], Train Loss: 0.0066, Val Loss: 0.0317\n",
      "Epoch [3/100], Train Loss: 0.0071, Val Loss: 0.0315\n",
      "Epoch [4/100], Train Loss: 0.0048, Val Loss: 0.0420\n",
      "Epoch [5/100], Train Loss: 0.0057, Val Loss: 0.0422\n",
      "Epoch [6/100], Train Loss: 0.0051, Val Loss: 0.0310\n",
      "Epoch [7/100], Train Loss: 0.0037, Val Loss: 0.0090\n",
      "Epoch [8/100], Train Loss: 0.0029, Val Loss: 0.0032\n",
      "Epoch [9/100], Train Loss: 0.0022, Val Loss: 0.0041\n",
      "Epoch [10/100], Train Loss: 0.0021, Val Loss: 0.0021\n",
      "Epoch [11/100], Train Loss: 0.0021, Val Loss: 0.0034\n",
      "Epoch [12/100], Train Loss: 0.0020, Val Loss: 0.0027\n",
      "Epoch [13/100], Train Loss: 0.0016, Val Loss: 0.0073\n",
      "Epoch [14/100], Train Loss: 0.0016, Val Loss: 0.0118\n",
      "Epoch [15/100], Train Loss: 0.0018, Val Loss: 0.0076\n",
      "Epoch [16/100], Train Loss: 0.0018, Val Loss: 0.0058\n",
      "Epoch [17/100], Train Loss: 0.0014, Val Loss: 0.0096\n",
      "Epoch [18/100], Train Loss: 0.0016, Val Loss: 0.0099\n",
      "Epoch [19/100], Train Loss: 0.0012, Val Loss: 0.0077\n",
      "Epoch [20/100], Train Loss: 0.0015, Val Loss: 0.0075\n",
      "Epoch [21/100], Train Loss: 0.0014, Val Loss: 0.0107\n",
      "Epoch [22/100], Train Loss: 0.0008, Val Loss: 0.0126\n",
      "Epoch [23/100], Train Loss: 0.0011, Val Loss: 0.0107\n",
      "Epoch [24/100], Train Loss: 0.0008, Val Loss: 0.0099\n",
      "Epoch [25/100], Train Loss: 0.0008, Val Loss: 0.0113\n",
      "Epoch [26/100], Train Loss: 0.0010, Val Loss: 0.0104\n",
      "Epoch [27/100], Train Loss: 0.0008, Val Loss: 0.0100\n",
      "Epoch [28/100], Train Loss: 0.0009, Val Loss: 0.0105\n",
      "Epoch [29/100], Train Loss: 0.0009, Val Loss: 0.0094\n",
      "Epoch [30/100], Train Loss: 0.0010, Val Loss: 0.0102\n",
      "Epoch [31/100], Train Loss: 0.0008, Val Loss: 0.0063\n",
      "Epoch [32/100], Train Loss: 0.0009, Val Loss: 0.0091\n",
      "Epoch [33/100], Train Loss: 0.0010, Val Loss: 0.0094\n",
      "Epoch [34/100], Train Loss: 0.0008, Val Loss: 0.0066\n",
      "Epoch [35/100], Train Loss: 0.0007, Val Loss: 0.0066\n",
      "Epoch [36/100], Train Loss: 0.0007, Val Loss: 0.0056\n",
      "Epoch [37/100], Train Loss: 0.0007, Val Loss: 0.0068\n",
      "Epoch [38/100], Train Loss: 0.0009, Val Loss: 0.0067\n",
      "Epoch [39/100], Train Loss: 0.0008, Val Loss: 0.0048\n",
      "Epoch [40/100], Train Loss: 0.0008, Val Loss: 0.0067\n",
      "Epoch [41/100], Train Loss: 0.0009, Val Loss: 0.0073\n",
      "Epoch [42/100], Train Loss: 0.0009, Val Loss: 0.0038\n",
      "Epoch [43/100], Train Loss: 0.0006, Val Loss: 0.0051\n",
      "Epoch [44/100], Train Loss: 0.0010, Val Loss: 0.0046\n",
      "Epoch [45/100], Train Loss: 0.0009, Val Loss: 0.0031\n",
      "Epoch [46/100], Train Loss: 0.0007, Val Loss: 0.0069\n",
      "Epoch [47/100], Train Loss: 0.0007, Val Loss: 0.0061\n",
      "Epoch [48/100], Train Loss: 0.0010, Val Loss: 0.0042\n",
      "Epoch [49/100], Train Loss: 0.0007, Val Loss: 0.0051\n",
      "Epoch [50/100], Train Loss: 0.0008, Val Loss: 0.0051\n",
      "Epoch [51/100], Train Loss: 0.0006, Val Loss: 0.0049\n",
      "Epoch [52/100], Train Loss: 0.0006, Val Loss: 0.0055\n",
      "Epoch [53/100], Train Loss: 0.0008, Val Loss: 0.0039\n",
      "Epoch [54/100], Train Loss: 0.0010, Val Loss: 0.0042\n",
      "Epoch [55/100], Train Loss: 0.0006, Val Loss: 0.0082\n",
      "Epoch [56/100], Train Loss: 0.0009, Val Loss: 0.0065\n",
      "Epoch [57/100], Train Loss: 0.0008, Val Loss: 0.0047\n",
      "Epoch [58/100], Train Loss: 0.0006, Val Loss: 0.0065\n",
      "Epoch [59/100], Train Loss: 0.0006, Val Loss: 0.0050\n",
      "Epoch [60/100], Train Loss: 0.0006, Val Loss: 0.0041\n",
      "Epoch [61/100], Train Loss: 0.0009, Val Loss: 0.0086\n",
      "Epoch [62/100], Train Loss: 0.0012, Val Loss: 0.0094\n",
      "Epoch [63/100], Train Loss: 0.0006, Val Loss: 0.0060\n",
      "Epoch [64/100], Train Loss: 0.0008, Val Loss: 0.0050\n",
      "Epoch [65/100], Train Loss: 0.0007, Val Loss: 0.0098\n",
      "Epoch [66/100], Train Loss: 0.0008, Val Loss: 0.0090\n",
      "Epoch [67/100], Train Loss: 0.0006, Val Loss: 0.0067\n",
      "Epoch [68/100], Train Loss: 0.0006, Val Loss: 0.0062\n",
      "Epoch [69/100], Train Loss: 0.0007, Val Loss: 0.0075\n",
      "Epoch [70/100], Train Loss: 0.0008, Val Loss: 0.0084\n",
      "Epoch [71/100], Train Loss: 0.0006, Val Loss: 0.0063\n",
      "Epoch [72/100], Train Loss: 0.0006, Val Loss: 0.0054\n",
      "Epoch [73/100], Train Loss: 0.0004, Val Loss: 0.0054\n",
      "Epoch [74/100], Train Loss: 0.0007, Val Loss: 0.0070\n",
      "Epoch [75/100], Train Loss: 0.0006, Val Loss: 0.0077\n",
      "Epoch [76/100], Train Loss: 0.0007, Val Loss: 0.0061\n",
      "Epoch [77/100], Train Loss: 0.0008, Val Loss: 0.0067\n",
      "Epoch [78/100], Train Loss: 0.0006, Val Loss: 0.0089\n",
      "Epoch [79/100], Train Loss: 0.0008, Val Loss: 0.0063\n",
      "Epoch [80/100], Train Loss: 0.0008, Val Loss: 0.0049\n",
      "Epoch [81/100], Train Loss: 0.0008, Val Loss: 0.0050\n",
      "Epoch [82/100], Train Loss: 0.0009, Val Loss: 0.0051\n",
      "Epoch [83/100], Train Loss: 0.0006, Val Loss: 0.0067\n",
      "Epoch [84/100], Train Loss: 0.0006, Val Loss: 0.0053\n",
      "Epoch [85/100], Train Loss: 0.0006, Val Loss: 0.0059\n",
      "Epoch [86/100], Train Loss: 0.0006, Val Loss: 0.0040\n",
      "Epoch [87/100], Train Loss: 0.0006, Val Loss: 0.0049\n",
      "Epoch [88/100], Train Loss: 0.0005, Val Loss: 0.0072\n",
      "Epoch [89/100], Train Loss: 0.0008, Val Loss: 0.0058\n",
      "Epoch [90/100], Train Loss: 0.0005, Val Loss: 0.0045\n",
      "Epoch [91/100], Train Loss: 0.0006, Val Loss: 0.0069\n",
      "Epoch [92/100], Train Loss: 0.0006, Val Loss: 0.0067\n",
      "Epoch [93/100], Train Loss: 0.0006, Val Loss: 0.0040\n",
      "Epoch [94/100], Train Loss: 0.0008, Val Loss: 0.0044\n",
      "Epoch [95/100], Train Loss: 0.0007, Val Loss: 0.0060\n",
      "Epoch [96/100], Train Loss: 0.0006, Val Loss: 0.0064\n",
      "Epoch [97/100], Train Loss: 0.0008, Val Loss: 0.0056\n",
      "Epoch [98/100], Train Loss: 0.0007, Val Loss: 0.0061\n",
      "Epoch [99/100], Train Loss: 0.0005, Val Loss: 0.0051\n",
      "Epoch [100/100], Train Loss: 0.0006, Val Loss: 0.0068\n",
      "\n",
      "Fold Results:\n",
      "fold: 1.0000\n",
      "mse: 0.0106\n",
      "rmse: 0.1031\n",
      "mae: 0.0822\n",
      "r2: 0.1241\n",
      "mape: 14.1267\n",
      "\n",
      "Fold 2/5\n",
      "Epoch [1/100], Train Loss: 0.0511, Val Loss: 0.1595\n",
      "Epoch [2/100], Train Loss: 0.0210, Val Loss: 0.0327\n",
      "Epoch [3/100], Train Loss: 0.0140, Val Loss: 0.0847\n",
      "Epoch [4/100], Train Loss: 0.0120, Val Loss: 0.0403\n",
      "Epoch [5/100], Train Loss: 0.0106, Val Loss: 0.0108\n",
      "Epoch [6/100], Train Loss: 0.0096, Val Loss: 0.0148\n",
      "Epoch [7/100], Train Loss: 0.0048, Val Loss: 0.0157\n",
      "Epoch [8/100], Train Loss: 0.0041, Val Loss: 0.0042\n",
      "Epoch [9/100], Train Loss: 0.0048, Val Loss: 0.0097\n",
      "Epoch [10/100], Train Loss: 0.0057, Val Loss: 0.0035\n",
      "Epoch [11/100], Train Loss: 0.0037, Val Loss: 0.0037\n",
      "Epoch [12/100], Train Loss: 0.0052, Val Loss: 0.0066\n",
      "Epoch [13/100], Train Loss: 0.0042, Val Loss: 0.0049\n",
      "Epoch [14/100], Train Loss: 0.0038, Val Loss: 0.0082\n",
      "Epoch [15/100], Train Loss: 0.0046, Val Loss: 0.0041\n",
      "Epoch [16/100], Train Loss: 0.0050, Val Loss: 0.0023\n",
      "Epoch [17/100], Train Loss: 0.0042, Val Loss: 0.0053\n",
      "Epoch [18/100], Train Loss: 0.0032, Val Loss: 0.0022\n",
      "Epoch [19/100], Train Loss: 0.0031, Val Loss: 0.0023\n",
      "Epoch [20/100], Train Loss: 0.0035, Val Loss: 0.0050\n",
      "Epoch [21/100], Train Loss: 0.0030, Val Loss: 0.0022\n",
      "Epoch [22/100], Train Loss: 0.0035, Val Loss: 0.0037\n",
      "Epoch [23/100], Train Loss: 0.0027, Val Loss: 0.0021\n",
      "Epoch [24/100], Train Loss: 0.0034, Val Loss: 0.0027\n",
      "Epoch [25/100], Train Loss: 0.0027, Val Loss: 0.0033\n",
      "Epoch [26/100], Train Loss: 0.0037, Val Loss: 0.0020\n",
      "Epoch [27/100], Train Loss: 0.0026, Val Loss: 0.0035\n",
      "Epoch [28/100], Train Loss: 0.0027, Val Loss: 0.0021\n",
      "Epoch [29/100], Train Loss: 0.0027, Val Loss: 0.0020\n",
      "Epoch [30/100], Train Loss: 0.0028, Val Loss: 0.0019\n",
      "Epoch [31/100], Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Epoch [32/100], Train Loss: 0.0025, Val Loss: 0.0018\n",
      "Epoch [33/100], Train Loss: 0.0036, Val Loss: 0.0023\n",
      "Epoch [34/100], Train Loss: 0.0025, Val Loss: 0.0026\n",
      "Epoch [35/100], Train Loss: 0.0037, Val Loss: 0.0024\n",
      "Epoch [36/100], Train Loss: 0.0027, Val Loss: 0.0018\n",
      "Epoch [37/100], Train Loss: 0.0026, Val Loss: 0.0019\n",
      "Epoch [38/100], Train Loss: 0.0028, Val Loss: 0.0033\n",
      "Epoch [39/100], Train Loss: 0.0030, Val Loss: 0.0019\n",
      "Epoch [40/100], Train Loss: 0.0033, Val Loss: 0.0026\n",
      "Epoch [41/100], Train Loss: 0.0029, Val Loss: 0.0023\n",
      "Epoch [42/100], Train Loss: 0.0022, Val Loss: 0.0016\n",
      "Epoch [43/100], Train Loss: 0.0036, Val Loss: 0.0038\n",
      "Epoch [44/100], Train Loss: 0.0023, Val Loss: 0.0017\n",
      "Epoch [45/100], Train Loss: 0.0037, Val Loss: 0.0032\n",
      "Epoch [46/100], Train Loss: 0.0019, Val Loss: 0.0020\n",
      "Epoch [47/100], Train Loss: 0.0026, Val Loss: 0.0018\n",
      "Epoch [48/100], Train Loss: 0.0034, Val Loss: 0.0016\n",
      "Epoch [49/100], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "Epoch [50/100], Train Loss: 0.0025, Val Loss: 0.0016\n",
      "Epoch [51/100], Train Loss: 0.0026, Val Loss: 0.0017\n",
      "Epoch [52/100], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "Epoch [53/100], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "Epoch [54/100], Train Loss: 0.0027, Val Loss: 0.0015\n",
      "Epoch [55/100], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "Epoch [56/100], Train Loss: 0.0027, Val Loss: 0.0032\n",
      "Epoch [57/100], Train Loss: 0.0024, Val Loss: 0.0021\n",
      "Epoch [58/100], Train Loss: 0.0034, Val Loss: 0.0015\n",
      "Epoch [59/100], Train Loss: 0.0026, Val Loss: 0.0022\n",
      "Epoch [60/100], Train Loss: 0.0031, Val Loss: 0.0022\n",
      "Epoch [61/100], Train Loss: 0.0029, Val Loss: 0.0019\n",
      "Epoch [62/100], Train Loss: 0.0019, Val Loss: 0.0028\n",
      "Epoch [63/100], Train Loss: 0.0017, Val Loss: 0.0017\n",
      "Epoch [64/100], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "Epoch [65/100], Train Loss: 0.0022, Val Loss: 0.0018\n",
      "Epoch [66/100], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Epoch [67/100], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "Epoch [68/100], Train Loss: 0.0025, Val Loss: 0.0015\n",
      "Epoch [69/100], Train Loss: 0.0030, Val Loss: 0.0017\n",
      "Epoch [70/100], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "Epoch [71/100], Train Loss: 0.0020, Val Loss: 0.0016\n",
      "Epoch [72/100], Train Loss: 0.0024, Val Loss: 0.0017\n",
      "Epoch [73/100], Train Loss: 0.0024, Val Loss: 0.0027\n",
      "Epoch [74/100], Train Loss: 0.0026, Val Loss: 0.0015\n",
      "Epoch [75/100], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "Epoch [76/100], Train Loss: 0.0016, Val Loss: 0.0020\n",
      "Epoch [77/100], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "Epoch [78/100], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "Epoch [79/100], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "Epoch [80/100], Train Loss: 0.0024, Val Loss: 0.0017\n",
      "Epoch [81/100], Train Loss: 0.0016, Val Loss: 0.0038\n",
      "Epoch [82/100], Train Loss: 0.0026, Val Loss: 0.0015\n",
      "Epoch [83/100], Train Loss: 0.0017, Val Loss: 0.0017\n",
      "Epoch [84/100], Train Loss: 0.0018, Val Loss: 0.0039\n",
      "Epoch [85/100], Train Loss: 0.0014, Val Loss: 0.0015\n",
      "Epoch [86/100], Train Loss: 0.0016, Val Loss: 0.0029\n",
      "Epoch [87/100], Train Loss: 0.0019, Val Loss: 0.0014\n",
      "Epoch [88/100], Train Loss: 0.0019, Val Loss: 0.0025\n",
      "Epoch [89/100], Train Loss: 0.0019, Val Loss: 0.0013\n",
      "Epoch [90/100], Train Loss: 0.0020, Val Loss: 0.0035\n",
      "Epoch [91/100], Train Loss: 0.0021, Val Loss: 0.0013\n",
      "Epoch [92/100], Train Loss: 0.0020, Val Loss: 0.0020\n",
      "Epoch [93/100], Train Loss: 0.0013, Val Loss: 0.0013\n",
      "Epoch [94/100], Train Loss: 0.0021, Val Loss: 0.0022\n",
      "Epoch [95/100], Train Loss: 0.0022, Val Loss: 0.0032\n",
      "Epoch [96/100], Train Loss: 0.0021, Val Loss: 0.0024\n",
      "Epoch [97/100], Train Loss: 0.0018, Val Loss: 0.0015\n",
      "Epoch [98/100], Train Loss: 0.0022, Val Loss: 0.0017\n",
      "Epoch [99/100], Train Loss: 0.0022, Val Loss: 0.0013\n",
      "Epoch [100/100], Train Loss: 0.0017, Val Loss: 0.0014\n",
      "\n",
      "Fold Results:\n",
      "fold: 2.0000\n",
      "mse: 0.0032\n",
      "rmse: 0.0561\n",
      "mae: 0.0435\n",
      "r2: 0.8279\n",
      "mape: 5.5486\n",
      "\n",
      "Fold 3/5\n",
      "Epoch [1/100], Train Loss: 0.1236, Val Loss: 0.1244\n",
      "Epoch [2/100], Train Loss: 0.0316, Val Loss: 0.0717\n",
      "Epoch [3/100], Train Loss: 0.0293, Val Loss: 0.0479\n",
      "Epoch [4/100], Train Loss: 0.0207, Val Loss: 0.0066\n",
      "Epoch [5/100], Train Loss: 0.0134, Val Loss: 0.0156\n",
      "Epoch [6/100], Train Loss: 0.0120, Val Loss: 0.0019\n",
      "Epoch [7/100], Train Loss: 0.0095, Val Loss: 0.0015\n",
      "Epoch [8/100], Train Loss: 0.0071, Val Loss: 0.0023\n",
      "Epoch [9/100], Train Loss: 0.0062, Val Loss: 0.0025\n",
      "Epoch [10/100], Train Loss: 0.0078, Val Loss: 0.0055\n",
      "Epoch [11/100], Train Loss: 0.0090, Val Loss: 0.0027\n",
      "Epoch [12/100], Train Loss: 0.0065, Val Loss: 0.0014\n",
      "Epoch [13/100], Train Loss: 0.0066, Val Loss: 0.0015\n",
      "Epoch [14/100], Train Loss: 0.0061, Val Loss: 0.0017\n",
      "Epoch [15/100], Train Loss: 0.0063, Val Loss: 0.0017\n",
      "Epoch [16/100], Train Loss: 0.0084, Val Loss: 0.0024\n",
      "Epoch [17/100], Train Loss: 0.0063, Val Loss: 0.0023\n",
      "Epoch [18/100], Train Loss: 0.0056, Val Loss: 0.0044\n",
      "Epoch [19/100], Train Loss: 0.0062, Val Loss: 0.0014\n",
      "Epoch [20/100], Train Loss: 0.0063, Val Loss: 0.0022\n",
      "Epoch [21/100], Train Loss: 0.0053, Val Loss: 0.0022\n",
      "Epoch [22/100], Train Loss: 0.0065, Val Loss: 0.0028\n",
      "Epoch [23/100], Train Loss: 0.0059, Val Loss: 0.0016\n",
      "Epoch [24/100], Train Loss: 0.0058, Val Loss: 0.0015\n",
      "Epoch [25/100], Train Loss: 0.0059, Val Loss: 0.0015\n",
      "Epoch [26/100], Train Loss: 0.0060, Val Loss: 0.0012\n",
      "Epoch [27/100], Train Loss: 0.0061, Val Loss: 0.0012\n",
      "Epoch [28/100], Train Loss: 0.0051, Val Loss: 0.0027\n",
      "Epoch [29/100], Train Loss: 0.0058, Val Loss: 0.0016\n",
      "Epoch [30/100], Train Loss: 0.0055, Val Loss: 0.0011\n",
      "Epoch [31/100], Train Loss: 0.0058, Val Loss: 0.0016\n",
      "Epoch [32/100], Train Loss: 0.0039, Val Loss: 0.0019\n",
      "Epoch [33/100], Train Loss: 0.0051, Val Loss: 0.0012\n",
      "Epoch [34/100], Train Loss: 0.0052, Val Loss: 0.0011\n",
      "Epoch [35/100], Train Loss: 0.0062, Val Loss: 0.0015\n",
      "Epoch [36/100], Train Loss: 0.0047, Val Loss: 0.0027\n",
      "Epoch [37/100], Train Loss: 0.0055, Val Loss: 0.0017\n",
      "Epoch [38/100], Train Loss: 0.0052, Val Loss: 0.0010\n",
      "Epoch [39/100], Train Loss: 0.0040, Val Loss: 0.0011\n",
      "Epoch [40/100], Train Loss: 0.0047, Val Loss: 0.0011\n",
      "Epoch [41/100], Train Loss: 0.0067, Val Loss: 0.0038\n",
      "Epoch [42/100], Train Loss: 0.0066, Val Loss: 0.0011\n",
      "Epoch [43/100], Train Loss: 0.0058, Val Loss: 0.0032\n",
      "Epoch [44/100], Train Loss: 0.0053, Val Loss: 0.0025\n",
      "Epoch [45/100], Train Loss: 0.0045, Val Loss: 0.0017\n",
      "Epoch [46/100], Train Loss: 0.0054, Val Loss: 0.0011\n",
      "Epoch [47/100], Train Loss: 0.0051, Val Loss: 0.0009\n",
      "Epoch [48/100], Train Loss: 0.0058, Val Loss: 0.0009\n",
      "Epoch [49/100], Train Loss: 0.0050, Val Loss: 0.0010\n",
      "Epoch [50/100], Train Loss: 0.0047, Val Loss: 0.0010\n",
      "Epoch [51/100], Train Loss: 0.0043, Val Loss: 0.0008\n",
      "Epoch [52/100], Train Loss: 0.0040, Val Loss: 0.0011\n",
      "Epoch [53/100], Train Loss: 0.0044, Val Loss: 0.0010\n",
      "Epoch [54/100], Train Loss: 0.0053, Val Loss: 0.0010\n",
      "Epoch [55/100], Train Loss: 0.0048, Val Loss: 0.0011\n",
      "Epoch [56/100], Train Loss: 0.0038, Val Loss: 0.0013\n",
      "Epoch [57/100], Train Loss: 0.0041, Val Loss: 0.0009\n",
      "Epoch [58/100], Train Loss: 0.0051, Val Loss: 0.0030\n",
      "Epoch [59/100], Train Loss: 0.0035, Val Loss: 0.0016\n",
      "Epoch [60/100], Train Loss: 0.0047, Val Loss: 0.0016\n",
      "Epoch [61/100], Train Loss: 0.0040, Val Loss: 0.0010\n",
      "Epoch [62/100], Train Loss: 0.0058, Val Loss: 0.0013\n",
      "Epoch [63/100], Train Loss: 0.0037, Val Loss: 0.0009\n",
      "Epoch [64/100], Train Loss: 0.0044, Val Loss: 0.0009\n",
      "Epoch [65/100], Train Loss: 0.0038, Val Loss: 0.0009\n",
      "Epoch [66/100], Train Loss: 0.0037, Val Loss: 0.0013\n",
      "Epoch [67/100], Train Loss: 0.0041, Val Loss: 0.0012\n",
      "Epoch [68/100], Train Loss: 0.0048, Val Loss: 0.0016\n",
      "Epoch [69/100], Train Loss: 0.0037, Val Loss: 0.0009\n",
      "Epoch [70/100], Train Loss: 0.0043, Val Loss: 0.0019\n",
      "Epoch [71/100], Train Loss: 0.0035, Val Loss: 0.0007\n",
      "Epoch [72/100], Train Loss: 0.0041, Val Loss: 0.0007\n",
      "Epoch [73/100], Train Loss: 0.0037, Val Loss: 0.0018\n",
      "Epoch [74/100], Train Loss: 0.0043, Val Loss: 0.0012\n",
      "Epoch [75/100], Train Loss: 0.0041, Val Loss: 0.0016\n",
      "Epoch [76/100], Train Loss: 0.0038, Val Loss: 0.0030\n",
      "Epoch [77/100], Train Loss: 0.0041, Val Loss: 0.0017\n",
      "Epoch [78/100], Train Loss: 0.0033, Val Loss: 0.0014\n",
      "Epoch [79/100], Train Loss: 0.0045, Val Loss: 0.0012\n",
      "Epoch [80/100], Train Loss: 0.0037, Val Loss: 0.0022\n",
      "Epoch [81/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "Epoch [82/100], Train Loss: 0.0039, Val Loss: 0.0006\n",
      "Epoch [83/100], Train Loss: 0.0042, Val Loss: 0.0040\n",
      "Epoch [84/100], Train Loss: 0.0049, Val Loss: 0.0007\n",
      "Epoch [85/100], Train Loss: 0.0038, Val Loss: 0.0012\n",
      "Epoch [86/100], Train Loss: 0.0039, Val Loss: 0.0018\n",
      "Epoch [87/100], Train Loss: 0.0041, Val Loss: 0.0006\n",
      "Epoch [88/100], Train Loss: 0.0041, Val Loss: 0.0006\n",
      "Epoch [89/100], Train Loss: 0.0033, Val Loss: 0.0006\n",
      "Epoch [90/100], Train Loss: 0.0045, Val Loss: 0.0012\n",
      "Epoch [91/100], Train Loss: 0.0037, Val Loss: 0.0009\n",
      "Epoch [92/100], Train Loss: 0.0036, Val Loss: 0.0009\n",
      "Epoch [93/100], Train Loss: 0.0043, Val Loss: 0.0009\n",
      "Epoch [94/100], Train Loss: 0.0051, Val Loss: 0.0006\n",
      "Epoch [95/100], Train Loss: 0.0034, Val Loss: 0.0008\n",
      "Epoch [96/100], Train Loss: 0.0038, Val Loss: 0.0017\n",
      "Epoch [97/100], Train Loss: 0.0037, Val Loss: 0.0008\n",
      "Epoch [98/100], Train Loss: 0.0042, Val Loss: 0.0010\n",
      "Epoch [99/100], Train Loss: 0.0042, Val Loss: 0.0032\n",
      "Epoch [100/100], Train Loss: 0.0031, Val Loss: 0.0021\n",
      "\n",
      "Fold Results:\n",
      "fold: 3.0000\n",
      "mse: 0.0038\n",
      "rmse: 0.0613\n",
      "mae: 0.0529\n",
      "r2: -0.2332\n",
      "mape: 6.0486\n",
      "\n",
      "Fold 4/5\n",
      "Epoch [1/100], Train Loss: 0.1214, Val Loss: 0.0089\n",
      "Epoch [2/100], Train Loss: 0.0325, Val Loss: 0.0027\n",
      "Epoch [3/100], Train Loss: 0.0173, Val Loss: 0.0042\n",
      "Epoch [4/100], Train Loss: 0.0112, Val Loss: 0.0028\n",
      "Epoch [5/100], Train Loss: 0.0112, Val Loss: 0.0067\n",
      "Epoch [6/100], Train Loss: 0.0123, Val Loss: 0.0037\n",
      "Epoch [7/100], Train Loss: 0.0114, Val Loss: 0.0051\n",
      "Epoch [8/100], Train Loss: 0.0110, Val Loss: 0.0021\n",
      "Epoch [9/100], Train Loss: 0.0072, Val Loss: 0.0021\n",
      "Epoch [10/100], Train Loss: 0.0084, Val Loss: 0.0035\n",
      "Epoch [11/100], Train Loss: 0.0090, Val Loss: 0.0019\n",
      "Epoch [12/100], Train Loss: 0.0101, Val Loss: 0.0034\n",
      "Epoch [13/100], Train Loss: 0.0103, Val Loss: 0.0054\n",
      "Epoch [14/100], Train Loss: 0.0112, Val Loss: 0.0018\n",
      "Epoch [15/100], Train Loss: 0.0103, Val Loss: 0.0030\n",
      "Epoch [16/100], Train Loss: 0.0107, Val Loss: 0.0050\n",
      "Epoch [17/100], Train Loss: 0.0127, Val Loss: 0.0021\n",
      "Epoch [18/100], Train Loss: 0.0082, Val Loss: 0.0043\n",
      "Epoch [19/100], Train Loss: 0.0092, Val Loss: 0.0037\n",
      "Epoch [20/100], Train Loss: 0.0081, Val Loss: 0.0018\n",
      "Epoch [21/100], Train Loss: 0.0072, Val Loss: 0.0019\n",
      "Epoch [22/100], Train Loss: 0.0079, Val Loss: 0.0020\n",
      "Epoch [23/100], Train Loss: 0.0092, Val Loss: 0.0017\n",
      "Epoch [24/100], Train Loss: 0.0088, Val Loss: 0.0035\n",
      "Epoch [25/100], Train Loss: 0.0096, Val Loss: 0.0019\n",
      "Epoch [26/100], Train Loss: 0.0083, Val Loss: 0.0018\n",
      "Epoch [27/100], Train Loss: 0.0086, Val Loss: 0.0027\n",
      "Epoch [28/100], Train Loss: 0.0082, Val Loss: 0.0015\n",
      "Epoch [29/100], Train Loss: 0.0089, Val Loss: 0.0017\n",
      "Epoch [30/100], Train Loss: 0.0070, Val Loss: 0.0014\n",
      "Epoch [31/100], Train Loss: 0.0072, Val Loss: 0.0025\n",
      "Epoch [32/100], Train Loss: 0.0076, Val Loss: 0.0014\n",
      "Epoch [33/100], Train Loss: 0.0087, Val Loss: 0.0014\n",
      "Epoch [34/100], Train Loss: 0.0090, Val Loss: 0.0017\n",
      "Epoch [35/100], Train Loss: 0.0082, Val Loss: 0.0013\n",
      "Epoch [36/100], Train Loss: 0.0085, Val Loss: 0.0013\n",
      "Epoch [37/100], Train Loss: 0.0086, Val Loss: 0.0035\n",
      "Epoch [38/100], Train Loss: 0.0078, Val Loss: 0.0024\n",
      "Epoch [39/100], Train Loss: 0.0088, Val Loss: 0.0012\n",
      "Epoch [40/100], Train Loss: 0.0081, Val Loss: 0.0012\n",
      "Epoch [41/100], Train Loss: 0.0075, Val Loss: 0.0013\n",
      "Epoch [42/100], Train Loss: 0.0070, Val Loss: 0.0015\n",
      "Epoch [43/100], Train Loss: 0.0083, Val Loss: 0.0012\n",
      "Epoch [44/100], Train Loss: 0.0078, Val Loss: 0.0013\n",
      "Epoch [45/100], Train Loss: 0.0058, Val Loss: 0.0013\n",
      "Epoch [46/100], Train Loss: 0.0064, Val Loss: 0.0010\n",
      "Epoch [47/100], Train Loss: 0.0057, Val Loss: 0.0015\n",
      "Epoch [48/100], Train Loss: 0.0068, Val Loss: 0.0010\n",
      "Epoch [49/100], Train Loss: 0.0057, Val Loss: 0.0011\n",
      "Epoch [50/100], Train Loss: 0.0069, Val Loss: 0.0010\n",
      "Epoch [51/100], Train Loss: 0.0061, Val Loss: 0.0010\n",
      "Epoch [52/100], Train Loss: 0.0068, Val Loss: 0.0010\n",
      "Epoch [53/100], Train Loss: 0.0060, Val Loss: 0.0016\n",
      "Epoch [54/100], Train Loss: 0.0060, Val Loss: 0.0015\n",
      "Epoch [55/100], Train Loss: 0.0063, Val Loss: 0.0037\n",
      "Epoch [56/100], Train Loss: 0.0061, Val Loss: 0.0015\n",
      "Epoch [57/100], Train Loss: 0.0069, Val Loss: 0.0027\n",
      "Epoch [58/100], Train Loss: 0.0068, Val Loss: 0.0024\n",
      "Epoch [59/100], Train Loss: 0.0061, Val Loss: 0.0011\n",
      "Epoch [60/100], Train Loss: 0.0063, Val Loss: 0.0010\n",
      "Epoch [61/100], Train Loss: 0.0062, Val Loss: 0.0010\n",
      "Epoch [62/100], Train Loss: 0.0072, Val Loss: 0.0019\n",
      "Epoch [63/100], Train Loss: 0.0072, Val Loss: 0.0033\n",
      "Epoch [64/100], Train Loss: 0.0072, Val Loss: 0.0009\n",
      "Epoch [65/100], Train Loss: 0.0053, Val Loss: 0.0009\n",
      "Epoch [66/100], Train Loss: 0.0043, Val Loss: 0.0012\n",
      "Epoch [67/100], Train Loss: 0.0050, Val Loss: 0.0010\n",
      "Epoch [68/100], Train Loss: 0.0053, Val Loss: 0.0012\n",
      "Epoch [69/100], Train Loss: 0.0060, Val Loss: 0.0011\n",
      "Epoch [70/100], Train Loss: 0.0064, Val Loss: 0.0022\n",
      "Epoch [71/100], Train Loss: 0.0059, Val Loss: 0.0008\n",
      "Epoch [72/100], Train Loss: 0.0053, Val Loss: 0.0010\n",
      "Epoch [73/100], Train Loss: 0.0069, Val Loss: 0.0022\n",
      "Epoch [74/100], Train Loss: 0.0063, Val Loss: 0.0008\n",
      "Epoch [75/100], Train Loss: 0.0059, Val Loss: 0.0008\n",
      "Epoch [76/100], Train Loss: 0.0049, Val Loss: 0.0019\n",
      "Epoch [77/100], Train Loss: 0.0054, Val Loss: 0.0008\n",
      "Epoch [78/100], Train Loss: 0.0054, Val Loss: 0.0011\n",
      "Epoch [79/100], Train Loss: 0.0048, Val Loss: 0.0015\n",
      "Epoch [80/100], Train Loss: 0.0053, Val Loss: 0.0010\n",
      "Epoch [81/100], Train Loss: 0.0065, Val Loss: 0.0023\n",
      "Epoch [82/100], Train Loss: 0.0051, Val Loss: 0.0023\n",
      "Epoch [83/100], Train Loss: 0.0050, Val Loss: 0.0011\n",
      "Epoch [84/100], Train Loss: 0.0062, Val Loss: 0.0008\n",
      "Epoch [85/100], Train Loss: 0.0051, Val Loss: 0.0010\n",
      "Epoch [86/100], Train Loss: 0.0048, Val Loss: 0.0008\n",
      "Epoch [87/100], Train Loss: 0.0048, Val Loss: 0.0007\n",
      "Epoch [88/100], Train Loss: 0.0050, Val Loss: 0.0009\n",
      "Epoch [89/100], Train Loss: 0.0041, Val Loss: 0.0007\n",
      "Epoch [90/100], Train Loss: 0.0042, Val Loss: 0.0007\n",
      "Epoch [91/100], Train Loss: 0.0046, Val Loss: 0.0007\n",
      "Epoch [92/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "Epoch [93/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "Epoch [94/100], Train Loss: 0.0048, Val Loss: 0.0008\n",
      "Epoch [95/100], Train Loss: 0.0043, Val Loss: 0.0008\n",
      "Epoch [96/100], Train Loss: 0.0044, Val Loss: 0.0011\n",
      "Epoch [97/100], Train Loss: 0.0046, Val Loss: 0.0009\n",
      "Epoch [98/100], Train Loss: 0.0045, Val Loss: 0.0013\n",
      "Epoch [99/100], Train Loss: 0.0044, Val Loss: 0.0019\n",
      "Epoch [100/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "\n",
      "Fold Results:\n",
      "fold: 4.0000\n",
      "mse: 0.0014\n",
      "rmse: 0.0378\n",
      "mae: 0.0290\n",
      "r2: 0.6877\n",
      "mape: 4.2509\n",
      "\n",
      "Fold 5/5\n",
      "Epoch [1/100], Train Loss: 0.1725, Val Loss: 0.0573\n",
      "Epoch [2/100], Train Loss: 0.0479, Val Loss: 0.0052\n",
      "Epoch [3/100], Train Loss: 0.0281, Val Loss: 0.0042\n",
      "Epoch [4/100], Train Loss: 0.0201, Val Loss: 0.0036\n",
      "Epoch [5/100], Train Loss: 0.0176, Val Loss: 0.0033\n",
      "Epoch [6/100], Train Loss: 0.0163, Val Loss: 0.0035\n",
      "Epoch [7/100], Train Loss: 0.0150, Val Loss: 0.0057\n",
      "Epoch [8/100], Train Loss: 0.0152, Val Loss: 0.0040\n",
      "Epoch [9/100], Train Loss: 0.0133, Val Loss: 0.0026\n",
      "Epoch [10/100], Train Loss: 0.0113, Val Loss: 0.0024\n",
      "Epoch [11/100], Train Loss: 0.0113, Val Loss: 0.0024\n",
      "Epoch [12/100], Train Loss: 0.0141, Val Loss: 0.0023\n",
      "Epoch [13/100], Train Loss: 0.0112, Val Loss: 0.0021\n",
      "Epoch [14/100], Train Loss: 0.0130, Val Loss: 0.0021\n",
      "Epoch [15/100], Train Loss: 0.0116, Val Loss: 0.0020\n",
      "Epoch [16/100], Train Loss: 0.0133, Val Loss: 0.0019\n",
      "Epoch [17/100], Train Loss: 0.0116, Val Loss: 0.0023\n",
      "Epoch [18/100], Train Loss: 0.0123, Val Loss: 0.0017\n",
      "Epoch [19/100], Train Loss: 0.0123, Val Loss: 0.0021\n",
      "Epoch [20/100], Train Loss: 0.0138, Val Loss: 0.0018\n",
      "Epoch [21/100], Train Loss: 0.0101, Val Loss: 0.0016\n",
      "Epoch [22/100], Train Loss: 0.0103, Val Loss: 0.0020\n",
      "Epoch [23/100], Train Loss: 0.0104, Val Loss: 0.0015\n",
      "Epoch [24/100], Train Loss: 0.0103, Val Loss: 0.0014\n",
      "Epoch [25/100], Train Loss: 0.0123, Val Loss: 0.0014\n",
      "Epoch [26/100], Train Loss: 0.0119, Val Loss: 0.0013\n",
      "Epoch [27/100], Train Loss: 0.0112, Val Loss: 0.0017\n",
      "Epoch [28/100], Train Loss: 0.0090, Val Loss: 0.0012\n",
      "Epoch [29/100], Train Loss: 0.0121, Val Loss: 0.0010\n",
      "Epoch [30/100], Train Loss: 0.0106, Val Loss: 0.0010\n",
      "Epoch [31/100], Train Loss: 0.0099, Val Loss: 0.0009\n",
      "Epoch [32/100], Train Loss: 0.0124, Val Loss: 0.0023\n",
      "Epoch [33/100], Train Loss: 0.0097, Val Loss: 0.0008\n",
      "Epoch [34/100], Train Loss: 0.0099, Val Loss: 0.0009\n",
      "Epoch [35/100], Train Loss: 0.0107, Val Loss: 0.0009\n",
      "Epoch [36/100], Train Loss: 0.0078, Val Loss: 0.0008\n",
      "Epoch [37/100], Train Loss: 0.0107, Val Loss: 0.0007\n",
      "Epoch [38/100], Train Loss: 0.0124, Val Loss: 0.0008\n",
      "Epoch [39/100], Train Loss: 0.0093, Val Loss: 0.0010\n",
      "Epoch [40/100], Train Loss: 0.0108, Val Loss: 0.0022\n",
      "Epoch [41/100], Train Loss: 0.0083, Val Loss: 0.0012\n",
      "Epoch [42/100], Train Loss: 0.0093, Val Loss: 0.0008\n",
      "Epoch [43/100], Train Loss: 0.0088, Val Loss: 0.0007\n",
      "Epoch [44/100], Train Loss: 0.0098, Val Loss: 0.0022\n",
      "Epoch [45/100], Train Loss: 0.0083, Val Loss: 0.0007\n",
      "Epoch [46/100], Train Loss: 0.0095, Val Loss: 0.0013\n",
      "Epoch [47/100], Train Loss: 0.0086, Val Loss: 0.0017\n",
      "Epoch [48/100], Train Loss: 0.0085, Val Loss: 0.0007\n",
      "Epoch [49/100], Train Loss: 0.0085, Val Loss: 0.0020\n",
      "Epoch [50/100], Train Loss: 0.0085, Val Loss: 0.0013\n",
      "Epoch [51/100], Train Loss: 0.0099, Val Loss: 0.0011\n",
      "Epoch [52/100], Train Loss: 0.0072, Val Loss: 0.0013\n",
      "Epoch [53/100], Train Loss: 0.0079, Val Loss: 0.0006\n",
      "Epoch [54/100], Train Loss: 0.0094, Val Loss: 0.0006\n",
      "Epoch [55/100], Train Loss: 0.0098, Val Loss: 0.0019\n",
      "Epoch [56/100], Train Loss: 0.0084, Val Loss: 0.0014\n",
      "Epoch [57/100], Train Loss: 0.0085, Val Loss: 0.0006\n",
      "Epoch [58/100], Train Loss: 0.0079, Val Loss: 0.0006\n",
      "Epoch [59/100], Train Loss: 0.0077, Val Loss: 0.0005\n",
      "Epoch [60/100], Train Loss: 0.0072, Val Loss: 0.0005\n",
      "Epoch [61/100], Train Loss: 0.0077, Val Loss: 0.0005\n",
      "Epoch [62/100], Train Loss: 0.0069, Val Loss: 0.0005\n",
      "Epoch [63/100], Train Loss: 0.0081, Val Loss: 0.0004\n",
      "Epoch [64/100], Train Loss: 0.0071, Val Loss: 0.0017\n",
      "Epoch [65/100], Train Loss: 0.0081, Val Loss: 0.0016\n",
      "Epoch [66/100], Train Loss: 0.0076, Val Loss: 0.0010\n",
      "Epoch [67/100], Train Loss: 0.0077, Val Loss: 0.0006\n",
      "Epoch [68/100], Train Loss: 0.0072, Val Loss: 0.0005\n",
      "Epoch [69/100], Train Loss: 0.0075, Val Loss: 0.0014\n",
      "Epoch [70/100], Train Loss: 0.0065, Val Loss: 0.0015\n",
      "Epoch [71/100], Train Loss: 0.0075, Val Loss: 0.0006\n",
      "Epoch [72/100], Train Loss: 0.0065, Val Loss: 0.0006\n",
      "Epoch [73/100], Train Loss: 0.0067, Val Loss: 0.0007\n",
      "Epoch [74/100], Train Loss: 0.0063, Val Loss: 0.0005\n",
      "Epoch [75/100], Train Loss: 0.0058, Val Loss: 0.0008\n",
      "Epoch [76/100], Train Loss: 0.0070, Val Loss: 0.0006\n",
      "Epoch [77/100], Train Loss: 0.0066, Val Loss: 0.0008\n",
      "Epoch [78/100], Train Loss: 0.0054, Val Loss: 0.0005\n",
      "Epoch [79/100], Train Loss: 0.0069, Val Loss: 0.0005\n",
      "Epoch [80/100], Train Loss: 0.0058, Val Loss: 0.0005\n",
      "Epoch [81/100], Train Loss: 0.0061, Val Loss: 0.0005\n",
      "Epoch [82/100], Train Loss: 0.0060, Val Loss: 0.0005\n",
      "Epoch [83/100], Train Loss: 0.0053, Val Loss: 0.0008\n",
      "Epoch [84/100], Train Loss: 0.0064, Val Loss: 0.0006\n",
      "Epoch [85/100], Train Loss: 0.0069, Val Loss: 0.0007\n",
      "Epoch [86/100], Train Loss: 0.0064, Val Loss: 0.0009\n",
      "Epoch [87/100], Train Loss: 0.0064, Val Loss: 0.0006\n",
      "Epoch [88/100], Train Loss: 0.0049, Val Loss: 0.0006\n",
      "Epoch [89/100], Train Loss: 0.0054, Val Loss: 0.0007\n",
      "Epoch [90/100], Train Loss: 0.0059, Val Loss: 0.0005\n",
      "Epoch [91/100], Train Loss: 0.0060, Val Loss: 0.0005\n",
      "Epoch [92/100], Train Loss: 0.0055, Val Loss: 0.0005\n",
      "Epoch [93/100], Train Loss: 0.0051, Val Loss: 0.0007\n",
      "Epoch [94/100], Train Loss: 0.0048, Val Loss: 0.0006\n",
      "Epoch [95/100], Train Loss: 0.0052, Val Loss: 0.0010\n",
      "Epoch [96/100], Train Loss: 0.0056, Val Loss: 0.0019\n",
      "Epoch [97/100], Train Loss: 0.0053, Val Loss: 0.0007\n",
      "Epoch [98/100], Train Loss: 0.0053, Val Loss: 0.0006\n",
      "Epoch [99/100], Train Loss: 0.0048, Val Loss: 0.0006\n",
      "Epoch [100/100], Train Loss: 0.0045, Val Loss: 0.0005\n",
      "\n",
      "Fold Results:\n",
      "fold: 5.0000\n",
      "mse: 0.0010\n",
      "rmse: 0.0312\n",
      "mae: 0.0241\n",
      "r2: 0.9220\n",
      "mape: 4.8456\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Average MSE: 0.0040\n",
      "Average RMSE: 0.0579\n",
      "Average MAE: 0.0464\n",
      "Average R2: 0.4657\n",
      "Average MAPE: 6.9641\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'sentiment']\n",
    "model, scaler, fold_metrics, avg_metrics = prepare_and_train_model(df, features, sequence_length=24, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254df69-d4bb-4012-8af3-934d90baa405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
