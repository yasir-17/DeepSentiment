{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74208f7e-6874-4c26-bfe0-581773405fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Using Pytorch Only #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2c9aba-0efe-49e2-b3c7-79e3a84bbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd58e9af-686e-4575-bea2-161c64693052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed_stock_data_aapl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e38fb959-ffe2-4afd-b40d-db68f6860ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/49488834/ipykernel_1776602/2596264522.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_data.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Prepare stock data\n",
    "def prepare_stock_data(df, ma_periods=[5, 10, 20, 50]):\n",
    "    data = df.copy()\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    for period in ma_periods:\n",
    "        data[f'MA_{period}'] = data['Adj Close'].rolling(window=period).mean()\n",
    "    data['Price_Change'] = data['Adj Close'].pct_change()\n",
    "    data['Volume_Change'] = data['Volume'].pct_change()\n",
    "    selected_features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'sentiment'] + \\\n",
    "                        [f'MA_{period}' for period in ma_periods]\n",
    "    processed_data = data[selected_features]\n",
    "    processed_data.dropna(inplace=True)\n",
    "    return processed_data\n",
    "\n",
    "df = prepare_stock_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bcf26a1-ad32-4706-8784-eebe76ccb3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim * 2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim * 2, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.dropout(self.relu(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db23a014-bfb3-451a-948b-8aed7b9684b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length), :])\n",
    "        y.append(data[i + sequence_length, 0])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd372dda-9a82-40b4-ba22-ff215fd08d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare and train model \n",
    "def prepare_and_train_model(df, features, sequence_length=20, test_size=0.2, learning_rate=0.001, epochs=100):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    X, y = create_sequences(scaled_data, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    # Model, loss, optimizer\n",
    "    input_dim = X_train.shape[2]\n",
    "    model = LSTMModel(input_dim=input_dim, hidden_dim=128, layer_dim=2, output_dim=1)\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output.view(-1), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_epoch_loss:.4f}')\n",
    "    \n",
    "    return model, scaler, (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f2c886e-8fc6-4fad-afd4-ccfcf3d3ab36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "def evaluate_stock_predictions(y_true, y_pred):\n",
    "    y_true, y_pred = y_true.detach().numpy(), y_pred.detach().numpy()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    results = {\n",
    "        'Mean Squared Error (MSE)': mse,\n",
    "        'Root Mean Squared Error (RMSE)': rmse,\n",
    "        'Mean Absolute Error (MAE)': mae,\n",
    "        'Mean Absolute Percentage Error (MAPE)': mape,\n",
    "        'R-squared (RÂ²)': r2\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61bfe294-f8db-43fb-956f-98045641dc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1282\n",
      "Epoch [2/100], Loss: 0.0276\n",
      "Epoch [3/100], Loss: 0.0164\n",
      "Epoch [4/100], Loss: 0.0134\n",
      "Epoch [5/100], Loss: 0.0094\n",
      "Epoch [6/100], Loss: 0.0108\n",
      "Epoch [7/100], Loss: 0.0094\n",
      "Epoch [8/100], Loss: 0.0100\n",
      "Epoch [9/100], Loss: 0.0099\n",
      "Epoch [10/100], Loss: 0.0091\n",
      "Epoch [11/100], Loss: 0.0105\n",
      "Epoch [12/100], Loss: 0.0110\n",
      "Epoch [13/100], Loss: 0.0105\n",
      "Epoch [14/100], Loss: 0.0104\n",
      "Epoch [15/100], Loss: 0.0109\n",
      "Epoch [16/100], Loss: 0.0098\n",
      "Epoch [17/100], Loss: 0.0108\n",
      "Epoch [18/100], Loss: 0.0090\n",
      "Epoch [19/100], Loss: 0.0076\n",
      "Epoch [20/100], Loss: 0.0098\n",
      "Epoch [21/100], Loss: 0.0091\n",
      "Epoch [22/100], Loss: 0.0098\n",
      "Epoch [23/100], Loss: 0.0099\n",
      "Epoch [24/100], Loss: 0.0089\n",
      "Epoch [25/100], Loss: 0.0098\n",
      "Epoch [26/100], Loss: 0.0089\n",
      "Epoch [27/100], Loss: 0.0091\n",
      "Epoch [28/100], Loss: 0.0083\n",
      "Epoch [29/100], Loss: 0.0093\n",
      "Epoch [30/100], Loss: 0.0090\n",
      "Epoch [31/100], Loss: 0.0094\n",
      "Epoch [32/100], Loss: 0.0086\n",
      "Epoch [33/100], Loss: 0.0091\n",
      "Epoch [34/100], Loss: 0.0077\n",
      "Epoch [35/100], Loss: 0.0086\n",
      "Epoch [36/100], Loss: 0.0101\n",
      "Epoch [37/100], Loss: 0.0096\n",
      "Epoch [38/100], Loss: 0.0099\n",
      "Epoch [39/100], Loss: 0.0070\n",
      "Epoch [40/100], Loss: 0.0065\n",
      "Epoch [41/100], Loss: 0.0082\n",
      "Epoch [42/100], Loss: 0.0063\n",
      "Epoch [43/100], Loss: 0.0088\n",
      "Epoch [44/100], Loss: 0.0076\n",
      "Epoch [45/100], Loss: 0.0075\n",
      "Epoch [46/100], Loss: 0.0077\n",
      "Epoch [47/100], Loss: 0.0064\n",
      "Epoch [48/100], Loss: 0.0072\n",
      "Epoch [49/100], Loss: 0.0067\n",
      "Epoch [50/100], Loss: 0.0068\n",
      "Epoch [51/100], Loss: 0.0064\n",
      "Epoch [52/100], Loss: 0.0072\n",
      "Epoch [53/100], Loss: 0.0071\n",
      "Epoch [54/100], Loss: 0.0072\n",
      "Epoch [55/100], Loss: 0.0061\n",
      "Epoch [56/100], Loss: 0.0080\n",
      "Epoch [57/100], Loss: 0.0057\n",
      "Epoch [58/100], Loss: 0.0067\n",
      "Epoch [59/100], Loss: 0.0059\n",
      "Epoch [60/100], Loss: 0.0064\n",
      "Epoch [61/100], Loss: 0.0065\n",
      "Epoch [62/100], Loss: 0.0057\n",
      "Epoch [63/100], Loss: 0.0075\n",
      "Epoch [64/100], Loss: 0.0068\n",
      "Epoch [65/100], Loss: 0.0059\n",
      "Epoch [66/100], Loss: 0.0057\n",
      "Epoch [67/100], Loss: 0.0063\n",
      "Epoch [68/100], Loss: 0.0060\n",
      "Epoch [69/100], Loss: 0.0060\n",
      "Epoch [70/100], Loss: 0.0055\n",
      "Epoch [71/100], Loss: 0.0050\n",
      "Epoch [72/100], Loss: 0.0066\n",
      "Epoch [73/100], Loss: 0.0074\n",
      "Epoch [74/100], Loss: 0.0054\n",
      "Epoch [75/100], Loss: 0.0054\n",
      "Epoch [76/100], Loss: 0.0054\n",
      "Epoch [77/100], Loss: 0.0059\n",
      "Epoch [78/100], Loss: 0.0053\n",
      "Epoch [79/100], Loss: 0.0052\n",
      "Epoch [80/100], Loss: 0.0053\n",
      "Epoch [81/100], Loss: 0.0041\n",
      "Epoch [82/100], Loss: 0.0053\n",
      "Epoch [83/100], Loss: 0.0045\n",
      "Epoch [84/100], Loss: 0.0051\n",
      "Epoch [85/100], Loss: 0.0043\n",
      "Epoch [86/100], Loss: 0.0051\n",
      "Epoch [87/100], Loss: 0.0055\n",
      "Epoch [88/100], Loss: 0.0050\n",
      "Epoch [89/100], Loss: 0.0046\n",
      "Epoch [90/100], Loss: 0.0045\n",
      "Epoch [91/100], Loss: 0.0049\n",
      "Epoch [92/100], Loss: 0.0046\n",
      "Epoch [93/100], Loss: 0.0042\n",
      "Epoch [94/100], Loss: 0.0040\n",
      "Epoch [95/100], Loss: 0.0045\n",
      "Epoch [96/100], Loss: 0.0046\n",
      "Epoch [97/100], Loss: 0.0042\n",
      "Epoch [98/100], Loss: 0.0042\n",
      "Epoch [99/100], Loss: 0.0044\n",
      "Epoch [100/100], Loss: 0.0043\n",
      "Mean Squared Error (MSE): 0.0010\n",
      "Root Mean Squared Error (RMSE): 0.0319\n",
      "Mean Absolute Error (MAE): 0.0246\n",
      "Mean Absolute Percentage Error (MAPE): 4.6656\n",
      "R-squared (RÂ²): 0.9460\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'sentiment']\n",
    "model, scaler, (X_train, X_test, y_train, y_test) = prepare_and_train_model(df, features, sequence_length=24)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "model.eval()\n",
    "predictions = model(X_test).view(-1).detach()\n",
    "results = evaluate_stock_predictions(y_test, predictions)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c868f0d-9035-4870-ad6f-c3a7dc9d428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5acf13-a624-4e81-99bb-e525b2edc006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0932\n",
      "Epoch [2/100], Loss: 0.0214\n",
      "Epoch [3/100], Loss: 0.0142\n",
      "Epoch [4/100], Loss: 0.0094\n",
      "Epoch [5/100], Loss: 0.0113\n",
      "Epoch [6/100], Loss: 0.0085\n",
      "Epoch [7/100], Loss: 0.0092\n",
      "Epoch [8/100], Loss: 0.0087\n",
      "Epoch [9/100], Loss: 0.0084\n",
      "Epoch [10/100], Loss: 0.0090\n",
      "Epoch [11/100], Loss: 0.0091\n",
      "Epoch [12/100], Loss: 0.0080\n",
      "Epoch [13/100], Loss: 0.0065\n",
      "Epoch [14/100], Loss: 0.0082\n",
      "Epoch [15/100], Loss: 0.0065\n",
      "Epoch [16/100], Loss: 0.0065\n",
      "Epoch [17/100], Loss: 0.0083\n",
      "Epoch [18/100], Loss: 0.0073\n",
      "Epoch [19/100], Loss: 0.0065\n",
      "Epoch [20/100], Loss: 0.0080\n",
      "Epoch [21/100], Loss: 0.0077\n",
      "Epoch [22/100], Loss: 0.0065\n",
      "Epoch [23/100], Loss: 0.0063\n",
      "Epoch [24/100], Loss: 0.0083\n",
      "Epoch [25/100], Loss: 0.0067\n",
      "Epoch [26/100], Loss: 0.0068\n",
      "Epoch [27/100], Loss: 0.0062\n",
      "Epoch [28/100], Loss: 0.0067\n",
      "Epoch [29/100], Loss: 0.0063\n",
      "Epoch [30/100], Loss: 0.0066\n",
      "Epoch [31/100], Loss: 0.0054\n",
      "Epoch [32/100], Loss: 0.0065\n",
      "Epoch [33/100], Loss: 0.0065\n",
      "Epoch [34/100], Loss: 0.0066\n",
      "Epoch [35/100], Loss: 0.0061\n",
      "Epoch [36/100], Loss: 0.0070\n",
      "Epoch [37/100], Loss: 0.0058\n",
      "Epoch [38/100], Loss: 0.0062\n",
      "Epoch [39/100], Loss: 0.0064\n",
      "Epoch [40/100], Loss: 0.0052\n",
      "Epoch [41/100], Loss: 0.0059\n",
      "Epoch [42/100], Loss: 0.0058\n",
      "Epoch [43/100], Loss: 0.0073\n",
      "Epoch [44/100], Loss: 0.0059\n",
      "Epoch [45/100], Loss: 0.0058\n",
      "Epoch [46/100], Loss: 0.0050\n",
      "Epoch [47/100], Loss: 0.0058\n",
      "Epoch [48/100], Loss: 0.0050\n",
      "Epoch [49/100], Loss: 0.0045\n",
      "Epoch [50/100], Loss: 0.0062\n",
      "Epoch [51/100], Loss: 0.0054\n",
      "Epoch [52/100], Loss: 0.0053\n",
      "Epoch [53/100], Loss: 0.0043\n",
      "Epoch [54/100], Loss: 0.0047\n",
      "Epoch [55/100], Loss: 0.0052\n",
      "Epoch [56/100], Loss: 0.0052\n",
      "Epoch [57/100], Loss: 0.0051\n",
      "Epoch [58/100], Loss: 0.0046\n",
      "Epoch [59/100], Loss: 0.0048\n",
      "Epoch [60/100], Loss: 0.0048\n",
      "Epoch [61/100], Loss: 0.0044\n",
      "Epoch [62/100], Loss: 0.0047\n",
      "Epoch [63/100], Loss: 0.0045\n",
      "Epoch [64/100], Loss: 0.0042\n",
      "Epoch [65/100], Loss: 0.0039\n",
      "Epoch [66/100], Loss: 0.0041\n",
      "Epoch [67/100], Loss: 0.0039\n",
      "Epoch [68/100], Loss: 0.0046\n",
      "Epoch [69/100], Loss: 0.0039\n",
      "Epoch [70/100], Loss: 0.0039\n",
      "Epoch [71/100], Loss: 0.0040\n",
      "Epoch [72/100], Loss: 0.0044\n",
      "Epoch [73/100], Loss: 0.0037\n",
      "Epoch [74/100], Loss: 0.0034\n",
      "Epoch [75/100], Loss: 0.0038\n",
      "Epoch [76/100], Loss: 0.0038\n",
      "Epoch [77/100], Loss: 0.0033\n",
      "Epoch [78/100], Loss: 0.0038\n",
      "Epoch [79/100], Loss: 0.0034\n",
      "Epoch [80/100], Loss: 0.0038\n",
      "Epoch [81/100], Loss: 0.0036\n",
      "Epoch [82/100], Loss: 0.0041\n",
      "Epoch [83/100], Loss: 0.0031\n",
      "Epoch [84/100], Loss: 0.0040\n",
      "Epoch [85/100], Loss: 0.0029\n",
      "Epoch [86/100], Loss: 0.0032\n",
      "Epoch [87/100], Loss: 0.0036\n",
      "Epoch [88/100], Loss: 0.0035\n",
      "Epoch [89/100], Loss: 0.0037\n",
      "Epoch [90/100], Loss: 0.0033\n",
      "Epoch [91/100], Loss: 0.0029\n",
      "Epoch [92/100], Loss: 0.0035\n",
      "Epoch [93/100], Loss: 0.0030\n",
      "Epoch [94/100], Loss: 0.0031\n",
      "Epoch [95/100], Loss: 0.0034\n",
      "Epoch [96/100], Loss: 0.0029\n",
      "Epoch [97/100], Loss: 0.0037\n",
      "Epoch [98/100], Loss: 0.0028\n",
      "Epoch [99/100], Loss: 0.0032\n",
      "Epoch [100/100], Loss: 0.0031\n",
      "Mean Squared Error (MSE): 0.0026\n",
      "Root Mean Squared Error (RMSE): 0.0512\n",
      "Mean Absolute Error (MAE): 0.0428\n",
      "Mean Absolute Percentage Error (MAPE): 8.1873\n",
      "R-squared (RÂ²): 0.8609\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50']\n",
    "model, scaler, (X_train, X_test, y_train, y_test) = prepare_and_train_model(df, features, sequence_length=24)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "model.eval()\n",
    "predictions = model(X_test).view(-1).detach()\n",
    "results = evaluate_stock_predictions(y_test, predictions)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c868e2e-a8c9-4620-beab-a451ef2dccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17c7ed-509f-4b9b-ab1b-40a24c8a8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Feature Importance ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea16c92d-5f76-4ea8-923d-11e1da110e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_permutation_importance(model, X, y, features, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Calculate permutation importance for LSTM model features\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained LSTM model\n",
    "    X: input tensor\n",
    "    y: target tensor\n",
    "    features: list of feature names\n",
    "    n_repeats: number of times to repeat permutation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    baseline_pred = model(X).view(-1)\n",
    "    baseline_loss = torch.nn.MSELoss()(baseline_pred, y)\n",
    "    \n",
    "    importance_scores = []\n",
    "    \n",
    "    for feature_idx in range(X.shape[2]):  # Loop through each feature\n",
    "        feature_importance = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X.clone()\n",
    "            # Permute the feature across all sequences\n",
    "            permuted_values = X_permuted[:, :, feature_idx][torch.randperm(X.shape[0])]\n",
    "            X_permuted[:, :, feature_idx] = permuted_values\n",
    "            \n",
    "            # Calculate new loss\n",
    "            with torch.no_grad():\n",
    "                new_pred = model(X_permuted).view(-1)\n",
    "                new_loss = torch.nn.MSELoss()(new_pred, y)\n",
    "            \n",
    "            # Importance is increase in loss\n",
    "            importance = (new_loss - baseline_loss).item()\n",
    "            feature_importance.append(importance)\n",
    "        \n",
    "        importance_scores.append(np.mean(feature_importance))\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importance_scores\n",
    "    })\n",
    "    return importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "def calculate_integrated_gradients(model, X, features, n_steps=50):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using integrated gradients\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained LSTM model\n",
    "    X: input tensor\n",
    "    features: list of feature names\n",
    "    n_steps: number of steps for path integral\n",
    "    \"\"\"\n",
    "    ig = IntegratedGradients(model)\n",
    "    baseline = torch.zeros_like(X)\n",
    "    \n",
    "    # Calculate attributions\n",
    "    attributions = ig.attribute(X, baseline, n_steps=n_steps)\n",
    "    \n",
    "    # Average attributions across sequences and samples\n",
    "    feature_importance = torch.mean(torch.abs(attributions), dim=(0, 1)).detach().numpy()\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "    return importance_df.sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9594c52-56ca-423a-9b8d-010e786774dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance_df, title):\n",
    "    \"\"\"\n",
    "    Plot feature importance results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def analyze_feature_importance(model, X_test, y_test, features):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained LSTM model\n",
    "    X_test: test input tensor\n",
    "    y_test: test target tensor\n",
    "    features: list of feature names\n",
    "    \"\"\"\n",
    "    # Calculate permutation importance\n",
    "    perm_importance = calculate_permutation_importance(model, X_test, y_test, features)\n",
    "    \n",
    "    # Calculate integrated gradients importance\n",
    "    ig_importance = calculate_integrated_gradients(model, X_test, features)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(perm_importance['feature'], perm_importance['importance'])\n",
    "    plt.title('Permutation Importance')\n",
    "    plt.xlabel('Increase in Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(ig_importance['feature'], ig_importance['importance'])\n",
    "    plt.title('Integrated Gradients Importance')\n",
    "    plt.xlabel('Attribution Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return {\n",
    "        'permutation': perm_importance,\n",
    "        'integrated_gradients': ig_importance\n",
    "    }\n",
    "\n",
    "def analyze_model_features(model, X_test, y_test, features):\n",
    "    \"\"\"\n",
    "    Wrapper function to analyze feature importance for your LSTM model\n",
    "    \"\"\"\n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    importance_results = analyze_feature_importance(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        features=features\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPermutation Importance:\")\n",
    "    print(importance_results['permutation'])\n",
    "    print(\"\\nIntegrated Gradients Importance:\")\n",
    "    print(importance_results['integrated_gradients'])\n",
    "    \n",
    "    return importance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eebe5986-1e3e-4da1-88f0-b8f4fc3e70e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation Importance:\n",
      "         feature  importance\n",
      "0      Adj Close    0.008302\n",
      "4           MA_5    0.004526\n",
      "5          MA_10    0.001303\n",
      "2   Price_Change    0.000927\n",
      "7          MA_50    0.000165\n",
      "3  Volume_Change    0.000054\n",
      "8      sentiment    0.000008\n",
      "6          MA_20   -0.000011\n",
      "1         Volume   -0.000037\n",
      "\n",
      "Integrated Gradients Importance:\n",
      "         feature  importance\n",
      "0      Adj Close    0.009401\n",
      "4           MA_5    0.006953\n",
      "2   Price_Change    0.004626\n",
      "5          MA_10    0.003763\n",
      "7          MA_50    0.001187\n",
      "3  Volume_Change    0.000612\n",
      "1         Volume    0.000422\n",
      "6          MA_20    0.000335\n",
      "8      sentiment    0.000260\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "importance_results = analyze_model_features(model, X_test, y_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648bc04-e17c-453e-ae3a-27b4667778e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd43b4-e297-44ed-aadb-6c9415cd2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Walk forward Cross Validation ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504bf9d6-b9e7-4590-a37c-a8c5937d06dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_and_train_model(df, features, sequence_length=20, n_splits=5, learning_rate=0.001, epochs=100):\n",
    "    \"\"\"\n",
    "    Prepare and train the model using walk-forward optimization with TimeSeriesSplit\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    X, y = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    # Lists to store performance metrics across folds\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Initialize best model tracking\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Walk-forward cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "        \n",
    "        # Initialize model, criterion, and optimizer\n",
    "        input_dim = X_train.shape[2]\n",
    "        model = LSTMModel(input_dim=input_dim, hidden_dim=128, layer_dim=2, output_dim=1)\n",
    "        criterion = nn.HuberLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Training loop for this fold\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_X)\n",
    "                loss = criterion(output.view(-1), batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_actuals = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    output = model(batch_X)\n",
    "                    val_loss += criterion(output.view(-1), batch_y).item()\n",
    "                    val_predictions.extend(output.view(-1).numpy())\n",
    "                    val_actuals.extend(batch_y.numpy())\n",
    "            \n",
    "            # Calculate average losses\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "            # Track best model based on validation loss\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict().copy()\n",
    "        \n",
    "        # Calculate fold metrics\n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_actuals = np.array(val_actuals)\n",
    "        fold_results = {\n",
    "            'fold': fold + 1,\n",
    "            'mse': mean_squared_error(val_actuals, val_predictions),\n",
    "            'rmse': np.sqrt(mean_squared_error(val_actuals, val_predictions)),\n",
    "            'mae': mean_absolute_error(val_actuals, val_predictions),\n",
    "            'r2': r2_score(val_actuals, val_predictions),\n",
    "            'mape': np.mean(np.abs((val_actuals - val_predictions) / val_actuals)) * 100\n",
    "        }\n",
    "        fold_metrics.append(fold_results)\n",
    "        \n",
    "        print(\"\\nFold Results:\")\n",
    "        for metric, value in fold_results.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Create final model with best parameters\n",
    "    final_model = LSTMModel(input_dim=input_dim, hidden_dim=128, layer_dim=2, output_dim=1)\n",
    "    final_model.load_state_dict(best_model)\n",
    "    \n",
    "    # Calculate and print average metrics across folds\n",
    "    print(\"\\nAverage Metrics Across All Folds:\")\n",
    "    avg_metrics = {}\n",
    "    metric_keys = ['mse', 'rmse', 'mae', 'r2', 'mape']\n",
    "    for metric in metric_keys:\n",
    "        avg_metrics[metric] = np.mean([fold[metric] for fold in fold_metrics])\n",
    "        print(f\"Average {metric.upper()}: {avg_metrics[metric]:.4f}\")\n",
    "    \n",
    "    return final_model, scaler, fold_metrics, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f21e09-c245-46ff-a7e0-89b75925b14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Epoch [1/100], Train Loss: 0.0107, Val Loss: 0.0615\n",
      "Epoch [2/100], Train Loss: 0.0066, Val Loss: 0.0317\n",
      "Epoch [3/100], Train Loss: 0.0071, Val Loss: 0.0315\n",
      "Epoch [4/100], Train Loss: 0.0048, Val Loss: 0.0420\n",
      "Epoch [5/100], Train Loss: 0.0057, Val Loss: 0.0422\n",
      "Epoch [6/100], Train Loss: 0.0051, Val Loss: 0.0310\n",
      "Epoch [7/100], Train Loss: 0.0037, Val Loss: 0.0090\n",
      "Epoch [8/100], Train Loss: 0.0029, Val Loss: 0.0032\n",
      "Epoch [9/100], Train Loss: 0.0022, Val Loss: 0.0041\n",
      "Epoch [10/100], Train Loss: 0.0021, Val Loss: 0.0021\n",
      "Epoch [11/100], Train Loss: 0.0021, Val Loss: 0.0034\n",
      "Epoch [12/100], Train Loss: 0.0020, Val Loss: 0.0027\n",
      "Epoch [13/100], Train Loss: 0.0016, Val Loss: 0.0073\n",
      "Epoch [14/100], Train Loss: 0.0016, Val Loss: 0.0118\n",
      "Epoch [15/100], Train Loss: 0.0018, Val Loss: 0.0076\n",
      "Epoch [16/100], Train Loss: 0.0018, Val Loss: 0.0058\n",
      "Epoch [17/100], Train Loss: 0.0014, Val Loss: 0.0096\n",
      "Epoch [18/100], Train Loss: 0.0016, Val Loss: 0.0099\n",
      "Epoch [19/100], Train Loss: 0.0012, Val Loss: 0.0077\n",
      "Epoch [20/100], Train Loss: 0.0015, Val Loss: 0.0075\n",
      "Epoch [21/100], Train Loss: 0.0014, Val Loss: 0.0107\n",
      "Epoch [22/100], Train Loss: 0.0008, Val Loss: 0.0126\n",
      "Epoch [23/100], Train Loss: 0.0011, Val Loss: 0.0107\n",
      "Epoch [24/100], Train Loss: 0.0008, Val Loss: 0.0099\n",
      "Epoch [25/100], Train Loss: 0.0008, Val Loss: 0.0113\n",
      "Epoch [26/100], Train Loss: 0.0010, Val Loss: 0.0104\n",
      "Epoch [27/100], Train Loss: 0.0008, Val Loss: 0.0100\n",
      "Epoch [28/100], Train Loss: 0.0009, Val Loss: 0.0105\n",
      "Epoch [29/100], Train Loss: 0.0009, Val Loss: 0.0094\n",
      "Epoch [30/100], Train Loss: 0.0010, Val Loss: 0.0102\n",
      "Epoch [31/100], Train Loss: 0.0008, Val Loss: 0.0063\n",
      "Epoch [32/100], Train Loss: 0.0009, Val Loss: 0.0091\n",
      "Epoch [33/100], Train Loss: 0.0010, Val Loss: 0.0094\n",
      "Epoch [34/100], Train Loss: 0.0008, Val Loss: 0.0066\n",
      "Epoch [35/100], Train Loss: 0.0007, Val Loss: 0.0066\n",
      "Epoch [36/100], Train Loss: 0.0007, Val Loss: 0.0056\n",
      "Epoch [37/100], Train Loss: 0.0007, Val Loss: 0.0068\n",
      "Epoch [38/100], Train Loss: 0.0009, Val Loss: 0.0067\n",
      "Epoch [39/100], Train Loss: 0.0008, Val Loss: 0.0048\n",
      "Epoch [40/100], Train Loss: 0.0008, Val Loss: 0.0067\n",
      "Epoch [41/100], Train Loss: 0.0009, Val Loss: 0.0073\n",
      "Epoch [42/100], Train Loss: 0.0009, Val Loss: 0.0038\n",
      "Epoch [43/100], Train Loss: 0.0006, Val Loss: 0.0051\n",
      "Epoch [44/100], Train Loss: 0.0010, Val Loss: 0.0046\n",
      "Epoch [45/100], Train Loss: 0.0009, Val Loss: 0.0031\n",
      "Epoch [46/100], Train Loss: 0.0007, Val Loss: 0.0069\n",
      "Epoch [47/100], Train Loss: 0.0007, Val Loss: 0.0061\n",
      "Epoch [48/100], Train Loss: 0.0010, Val Loss: 0.0042\n",
      "Epoch [49/100], Train Loss: 0.0007, Val Loss: 0.0051\n",
      "Epoch [50/100], Train Loss: 0.0008, Val Loss: 0.0051\n",
      "Epoch [51/100], Train Loss: 0.0006, Val Loss: 0.0049\n",
      "Epoch [52/100], Train Loss: 0.0006, Val Loss: 0.0055\n",
      "Epoch [53/100], Train Loss: 0.0008, Val Loss: 0.0039\n",
      "Epoch [54/100], Train Loss: 0.0010, Val Loss: 0.0042\n",
      "Epoch [55/100], Train Loss: 0.0006, Val Loss: 0.0082\n",
      "Epoch [56/100], Train Loss: 0.0009, Val Loss: 0.0065\n",
      "Epoch [57/100], Train Loss: 0.0008, Val Loss: 0.0047\n",
      "Epoch [58/100], Train Loss: 0.0006, Val Loss: 0.0065\n",
      "Epoch [59/100], Train Loss: 0.0006, Val Loss: 0.0050\n",
      "Epoch [60/100], Train Loss: 0.0006, Val Loss: 0.0041\n",
      "Epoch [61/100], Train Loss: 0.0009, Val Loss: 0.0086\n",
      "Epoch [62/100], Train Loss: 0.0012, Val Loss: 0.0094\n",
      "Epoch [63/100], Train Loss: 0.0006, Val Loss: 0.0060\n",
      "Epoch [64/100], Train Loss: 0.0008, Val Loss: 0.0050\n",
      "Epoch [65/100], Train Loss: 0.0007, Val Loss: 0.0098\n",
      "Epoch [66/100], Train Loss: 0.0008, Val Loss: 0.0090\n",
      "Epoch [67/100], Train Loss: 0.0006, Val Loss: 0.0067\n",
      "Epoch [68/100], Train Loss: 0.0006, Val Loss: 0.0062\n",
      "Epoch [69/100], Train Loss: 0.0007, Val Loss: 0.0075\n",
      "Epoch [70/100], Train Loss: 0.0008, Val Loss: 0.0084\n",
      "Epoch [71/100], Train Loss: 0.0006, Val Loss: 0.0063\n",
      "Epoch [72/100], Train Loss: 0.0006, Val Loss: 0.0054\n",
      "Epoch [73/100], Train Loss: 0.0004, Val Loss: 0.0054\n",
      "Epoch [74/100], Train Loss: 0.0007, Val Loss: 0.0070\n",
      "Epoch [75/100], Train Loss: 0.0006, Val Loss: 0.0077\n",
      "Epoch [76/100], Train Loss: 0.0007, Val Loss: 0.0061\n",
      "Epoch [77/100], Train Loss: 0.0008, Val Loss: 0.0067\n",
      "Epoch [78/100], Train Loss: 0.0006, Val Loss: 0.0089\n",
      "Epoch [79/100], Train Loss: 0.0008, Val Loss: 0.0063\n",
      "Epoch [80/100], Train Loss: 0.0008, Val Loss: 0.0049\n",
      "Epoch [81/100], Train Loss: 0.0008, Val Loss: 0.0050\n",
      "Epoch [82/100], Train Loss: 0.0009, Val Loss: 0.0051\n",
      "Epoch [83/100], Train Loss: 0.0006, Val Loss: 0.0067\n",
      "Epoch [84/100], Train Loss: 0.0006, Val Loss: 0.0053\n",
      "Epoch [85/100], Train Loss: 0.0006, Val Loss: 0.0059\n",
      "Epoch [86/100], Train Loss: 0.0006, Val Loss: 0.0040\n",
      "Epoch [87/100], Train Loss: 0.0006, Val Loss: 0.0049\n",
      "Epoch [88/100], Train Loss: 0.0005, Val Loss: 0.0072\n",
      "Epoch [89/100], Train Loss: 0.0008, Val Loss: 0.0058\n",
      "Epoch [90/100], Train Loss: 0.0005, Val Loss: 0.0045\n",
      "Epoch [91/100], Train Loss: 0.0006, Val Loss: 0.0069\n",
      "Epoch [92/100], Train Loss: 0.0006, Val Loss: 0.0067\n",
      "Epoch [93/100], Train Loss: 0.0006, Val Loss: 0.0040\n",
      "Epoch [94/100], Train Loss: 0.0008, Val Loss: 0.0044\n",
      "Epoch [95/100], Train Loss: 0.0007, Val Loss: 0.0060\n",
      "Epoch [96/100], Train Loss: 0.0006, Val Loss: 0.0064\n",
      "Epoch [97/100], Train Loss: 0.0008, Val Loss: 0.0056\n",
      "Epoch [98/100], Train Loss: 0.0007, Val Loss: 0.0061\n",
      "Epoch [99/100], Train Loss: 0.0005, Val Loss: 0.0051\n",
      "Epoch [100/100], Train Loss: 0.0006, Val Loss: 0.0068\n",
      "\n",
      "Fold Results:\n",
      "fold: 1.0000\n",
      "mse: 0.0106\n",
      "rmse: 0.1031\n",
      "mae: 0.0822\n",
      "r2: 0.1241\n",
      "mape: 14.1267\n",
      "\n",
      "Fold 2/5\n",
      "Epoch [1/100], Train Loss: 0.0511, Val Loss: 0.1595\n",
      "Epoch [2/100], Train Loss: 0.0210, Val Loss: 0.0327\n",
      "Epoch [3/100], Train Loss: 0.0140, Val Loss: 0.0847\n",
      "Epoch [4/100], Train Loss: 0.0120, Val Loss: 0.0403\n",
      "Epoch [5/100], Train Loss: 0.0106, Val Loss: 0.0108\n",
      "Epoch [6/100], Train Loss: 0.0096, Val Loss: 0.0148\n",
      "Epoch [7/100], Train Loss: 0.0048, Val Loss: 0.0157\n",
      "Epoch [8/100], Train Loss: 0.0041, Val Loss: 0.0042\n",
      "Epoch [9/100], Train Loss: 0.0048, Val Loss: 0.0097\n",
      "Epoch [10/100], Train Loss: 0.0057, Val Loss: 0.0035\n",
      "Epoch [11/100], Train Loss: 0.0037, Val Loss: 0.0037\n",
      "Epoch [12/100], Train Loss: 0.0052, Val Loss: 0.0066\n",
      "Epoch [13/100], Train Loss: 0.0042, Val Loss: 0.0049\n",
      "Epoch [14/100], Train Loss: 0.0038, Val Loss: 0.0082\n",
      "Epoch [15/100], Train Loss: 0.0046, Val Loss: 0.0041\n",
      "Epoch [16/100], Train Loss: 0.0050, Val Loss: 0.0023\n",
      "Epoch [17/100], Train Loss: 0.0042, Val Loss: 0.0053\n",
      "Epoch [18/100], Train Loss: 0.0032, Val Loss: 0.0022\n",
      "Epoch [19/100], Train Loss: 0.0031, Val Loss: 0.0023\n",
      "Epoch [20/100], Train Loss: 0.0035, Val Loss: 0.0050\n",
      "Epoch [21/100], Train Loss: 0.0030, Val Loss: 0.0022\n",
      "Epoch [22/100], Train Loss: 0.0035, Val Loss: 0.0037\n",
      "Epoch [23/100], Train Loss: 0.0027, Val Loss: 0.0021\n",
      "Epoch [24/100], Train Loss: 0.0034, Val Loss: 0.0027\n",
      "Epoch [25/100], Train Loss: 0.0027, Val Loss: 0.0033\n",
      "Epoch [26/100], Train Loss: 0.0037, Val Loss: 0.0020\n",
      "Epoch [27/100], Train Loss: 0.0026, Val Loss: 0.0035\n",
      "Epoch [28/100], Train Loss: 0.0027, Val Loss: 0.0021\n",
      "Epoch [29/100], Train Loss: 0.0027, Val Loss: 0.0020\n",
      "Epoch [30/100], Train Loss: 0.0028, Val Loss: 0.0019\n",
      "Epoch [31/100], Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Epoch [32/100], Train Loss: 0.0025, Val Loss: 0.0018\n",
      "Epoch [33/100], Train Loss: 0.0036, Val Loss: 0.0023\n",
      "Epoch [34/100], Train Loss: 0.0025, Val Loss: 0.0026\n",
      "Epoch [35/100], Train Loss: 0.0037, Val Loss: 0.0024\n",
      "Epoch [36/100], Train Loss: 0.0027, Val Loss: 0.0018\n",
      "Epoch [37/100], Train Loss: 0.0026, Val Loss: 0.0019\n",
      "Epoch [38/100], Train Loss: 0.0028, Val Loss: 0.0033\n",
      "Epoch [39/100], Train Loss: 0.0030, Val Loss: 0.0019\n",
      "Epoch [40/100], Train Loss: 0.0033, Val Loss: 0.0026\n",
      "Epoch [41/100], Train Loss: 0.0029, Val Loss: 0.0023\n",
      "Epoch [42/100], Train Loss: 0.0022, Val Loss: 0.0016\n",
      "Epoch [43/100], Train Loss: 0.0036, Val Loss: 0.0038\n",
      "Epoch [44/100], Train Loss: 0.0023, Val Loss: 0.0017\n",
      "Epoch [45/100], Train Loss: 0.0037, Val Loss: 0.0032\n",
      "Epoch [46/100], Train Loss: 0.0019, Val Loss: 0.0020\n",
      "Epoch [47/100], Train Loss: 0.0026, Val Loss: 0.0018\n",
      "Epoch [48/100], Train Loss: 0.0034, Val Loss: 0.0016\n",
      "Epoch [49/100], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "Epoch [50/100], Train Loss: 0.0025, Val Loss: 0.0016\n",
      "Epoch [51/100], Train Loss: 0.0026, Val Loss: 0.0017\n",
      "Epoch [52/100], Train Loss: 0.0020, Val Loss: 0.0018\n",
      "Epoch [53/100], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "Epoch [54/100], Train Loss: 0.0027, Val Loss: 0.0015\n",
      "Epoch [55/100], Train Loss: 0.0022, Val Loss: 0.0019\n",
      "Epoch [56/100], Train Loss: 0.0027, Val Loss: 0.0032\n",
      "Epoch [57/100], Train Loss: 0.0024, Val Loss: 0.0021\n",
      "Epoch [58/100], Train Loss: 0.0034, Val Loss: 0.0015\n",
      "Epoch [59/100], Train Loss: 0.0026, Val Loss: 0.0022\n",
      "Epoch [60/100], Train Loss: 0.0031, Val Loss: 0.0022\n",
      "Epoch [61/100], Train Loss: 0.0029, Val Loss: 0.0019\n",
      "Epoch [62/100], Train Loss: 0.0019, Val Loss: 0.0028\n",
      "Epoch [63/100], Train Loss: 0.0017, Val Loss: 0.0017\n",
      "Epoch [64/100], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "Epoch [65/100], Train Loss: 0.0022, Val Loss: 0.0018\n",
      "Epoch [66/100], Train Loss: 0.0023, Val Loss: 0.0036\n",
      "Epoch [67/100], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "Epoch [68/100], Train Loss: 0.0025, Val Loss: 0.0015\n",
      "Epoch [69/100], Train Loss: 0.0030, Val Loss: 0.0017\n",
      "Epoch [70/100], Train Loss: 0.0023, Val Loss: 0.0016\n",
      "Epoch [71/100], Train Loss: 0.0020, Val Loss: 0.0016\n",
      "Epoch [72/100], Train Loss: 0.0024, Val Loss: 0.0017\n",
      "Epoch [73/100], Train Loss: 0.0024, Val Loss: 0.0027\n",
      "Epoch [74/100], Train Loss: 0.0026, Val Loss: 0.0015\n",
      "Epoch [75/100], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "Epoch [76/100], Train Loss: 0.0016, Val Loss: 0.0020\n",
      "Epoch [77/100], Train Loss: 0.0021, Val Loss: 0.0017\n",
      "Epoch [78/100], Train Loss: 0.0021, Val Loss: 0.0020\n",
      "Epoch [79/100], Train Loss: 0.0024, Val Loss: 0.0031\n",
      "Epoch [80/100], Train Loss: 0.0024, Val Loss: 0.0017\n",
      "Epoch [81/100], Train Loss: 0.0016, Val Loss: 0.0038\n",
      "Epoch [82/100], Train Loss: 0.0026, Val Loss: 0.0015\n",
      "Epoch [83/100], Train Loss: 0.0017, Val Loss: 0.0017\n",
      "Epoch [84/100], Train Loss: 0.0018, Val Loss: 0.0039\n",
      "Epoch [85/100], Train Loss: 0.0014, Val Loss: 0.0015\n",
      "Epoch [86/100], Train Loss: 0.0016, Val Loss: 0.0029\n",
      "Epoch [87/100], Train Loss: 0.0019, Val Loss: 0.0014\n",
      "Epoch [88/100], Train Loss: 0.0019, Val Loss: 0.0025\n",
      "Epoch [89/100], Train Loss: 0.0019, Val Loss: 0.0013\n",
      "Epoch [90/100], Train Loss: 0.0020, Val Loss: 0.0035\n",
      "Epoch [91/100], Train Loss: 0.0021, Val Loss: 0.0013\n",
      "Epoch [92/100], Train Loss: 0.0020, Val Loss: 0.0020\n",
      "Epoch [93/100], Train Loss: 0.0013, Val Loss: 0.0013\n",
      "Epoch [94/100], Train Loss: 0.0021, Val Loss: 0.0022\n",
      "Epoch [95/100], Train Loss: 0.0022, Val Loss: 0.0032\n",
      "Epoch [96/100], Train Loss: 0.0021, Val Loss: 0.0024\n",
      "Epoch [97/100], Train Loss: 0.0018, Val Loss: 0.0015\n",
      "Epoch [98/100], Train Loss: 0.0022, Val Loss: 0.0017\n",
      "Epoch [99/100], Train Loss: 0.0022, Val Loss: 0.0013\n",
      "Epoch [100/100], Train Loss: 0.0017, Val Loss: 0.0014\n",
      "\n",
      "Fold Results:\n",
      "fold: 2.0000\n",
      "mse: 0.0032\n",
      "rmse: 0.0561\n",
      "mae: 0.0435\n",
      "r2: 0.8279\n",
      "mape: 5.5486\n",
      "\n",
      "Fold 3/5\n",
      "Epoch [1/100], Train Loss: 0.1236, Val Loss: 0.1244\n",
      "Epoch [2/100], Train Loss: 0.0316, Val Loss: 0.0717\n",
      "Epoch [3/100], Train Loss: 0.0293, Val Loss: 0.0479\n",
      "Epoch [4/100], Train Loss: 0.0207, Val Loss: 0.0066\n",
      "Epoch [5/100], Train Loss: 0.0134, Val Loss: 0.0156\n",
      "Epoch [6/100], Train Loss: 0.0120, Val Loss: 0.0019\n",
      "Epoch [7/100], Train Loss: 0.0095, Val Loss: 0.0015\n",
      "Epoch [8/100], Train Loss: 0.0071, Val Loss: 0.0023\n",
      "Epoch [9/100], Train Loss: 0.0062, Val Loss: 0.0025\n",
      "Epoch [10/100], Train Loss: 0.0078, Val Loss: 0.0055\n",
      "Epoch [11/100], Train Loss: 0.0090, Val Loss: 0.0027\n",
      "Epoch [12/100], Train Loss: 0.0065, Val Loss: 0.0014\n",
      "Epoch [13/100], Train Loss: 0.0066, Val Loss: 0.0015\n",
      "Epoch [14/100], Train Loss: 0.0061, Val Loss: 0.0017\n",
      "Epoch [15/100], Train Loss: 0.0063, Val Loss: 0.0017\n",
      "Epoch [16/100], Train Loss: 0.0084, Val Loss: 0.0024\n",
      "Epoch [17/100], Train Loss: 0.0063, Val Loss: 0.0023\n",
      "Epoch [18/100], Train Loss: 0.0056, Val Loss: 0.0044\n",
      "Epoch [19/100], Train Loss: 0.0062, Val Loss: 0.0014\n",
      "Epoch [20/100], Train Loss: 0.0063, Val Loss: 0.0022\n",
      "Epoch [21/100], Train Loss: 0.0053, Val Loss: 0.0022\n",
      "Epoch [22/100], Train Loss: 0.0065, Val Loss: 0.0028\n",
      "Epoch [23/100], Train Loss: 0.0059, Val Loss: 0.0016\n",
      "Epoch [24/100], Train Loss: 0.0058, Val Loss: 0.0015\n",
      "Epoch [25/100], Train Loss: 0.0059, Val Loss: 0.0015\n",
      "Epoch [26/100], Train Loss: 0.0060, Val Loss: 0.0012\n",
      "Epoch [27/100], Train Loss: 0.0061, Val Loss: 0.0012\n",
      "Epoch [28/100], Train Loss: 0.0051, Val Loss: 0.0027\n",
      "Epoch [29/100], Train Loss: 0.0058, Val Loss: 0.0016\n",
      "Epoch [30/100], Train Loss: 0.0055, Val Loss: 0.0011\n",
      "Epoch [31/100], Train Loss: 0.0058, Val Loss: 0.0016\n",
      "Epoch [32/100], Train Loss: 0.0039, Val Loss: 0.0019\n",
      "Epoch [33/100], Train Loss: 0.0051, Val Loss: 0.0012\n",
      "Epoch [34/100], Train Loss: 0.0052, Val Loss: 0.0011\n",
      "Epoch [35/100], Train Loss: 0.0062, Val Loss: 0.0015\n",
      "Epoch [36/100], Train Loss: 0.0047, Val Loss: 0.0027\n",
      "Epoch [37/100], Train Loss: 0.0055, Val Loss: 0.0017\n",
      "Epoch [38/100], Train Loss: 0.0052, Val Loss: 0.0010\n",
      "Epoch [39/100], Train Loss: 0.0040, Val Loss: 0.0011\n",
      "Epoch [40/100], Train Loss: 0.0047, Val Loss: 0.0011\n",
      "Epoch [41/100], Train Loss: 0.0067, Val Loss: 0.0038\n",
      "Epoch [42/100], Train Loss: 0.0066, Val Loss: 0.0011\n",
      "Epoch [43/100], Train Loss: 0.0058, Val Loss: 0.0032\n",
      "Epoch [44/100], Train Loss: 0.0053, Val Loss: 0.0025\n",
      "Epoch [45/100], Train Loss: 0.0045, Val Loss: 0.0017\n",
      "Epoch [46/100], Train Loss: 0.0054, Val Loss: 0.0011\n",
      "Epoch [47/100], Train Loss: 0.0051, Val Loss: 0.0009\n",
      "Epoch [48/100], Train Loss: 0.0058, Val Loss: 0.0009\n",
      "Epoch [49/100], Train Loss: 0.0050, Val Loss: 0.0010\n",
      "Epoch [50/100], Train Loss: 0.0047, Val Loss: 0.0010\n",
      "Epoch [51/100], Train Loss: 0.0043, Val Loss: 0.0008\n",
      "Epoch [52/100], Train Loss: 0.0040, Val Loss: 0.0011\n",
      "Epoch [53/100], Train Loss: 0.0044, Val Loss: 0.0010\n",
      "Epoch [54/100], Train Loss: 0.0053, Val Loss: 0.0010\n",
      "Epoch [55/100], Train Loss: 0.0048, Val Loss: 0.0011\n",
      "Epoch [56/100], Train Loss: 0.0038, Val Loss: 0.0013\n",
      "Epoch [57/100], Train Loss: 0.0041, Val Loss: 0.0009\n",
      "Epoch [58/100], Train Loss: 0.0051, Val Loss: 0.0030\n",
      "Epoch [59/100], Train Loss: 0.0035, Val Loss: 0.0016\n",
      "Epoch [60/100], Train Loss: 0.0047, Val Loss: 0.0016\n",
      "Epoch [61/100], Train Loss: 0.0040, Val Loss: 0.0010\n",
      "Epoch [62/100], Train Loss: 0.0058, Val Loss: 0.0013\n",
      "Epoch [63/100], Train Loss: 0.0037, Val Loss: 0.0009\n",
      "Epoch [64/100], Train Loss: 0.0044, Val Loss: 0.0009\n",
      "Epoch [65/100], Train Loss: 0.0038, Val Loss: 0.0009\n",
      "Epoch [66/100], Train Loss: 0.0037, Val Loss: 0.0013\n",
      "Epoch [67/100], Train Loss: 0.0041, Val Loss: 0.0012\n",
      "Epoch [68/100], Train Loss: 0.0048, Val Loss: 0.0016\n",
      "Epoch [69/100], Train Loss: 0.0037, Val Loss: 0.0009\n",
      "Epoch [70/100], Train Loss: 0.0043, Val Loss: 0.0019\n",
      "Epoch [71/100], Train Loss: 0.0035, Val Loss: 0.0007\n",
      "Epoch [72/100], Train Loss: 0.0041, Val Loss: 0.0007\n",
      "Epoch [73/100], Train Loss: 0.0037, Val Loss: 0.0018\n",
      "Epoch [74/100], Train Loss: 0.0043, Val Loss: 0.0012\n",
      "Epoch [75/100], Train Loss: 0.0041, Val Loss: 0.0016\n",
      "Epoch [76/100], Train Loss: 0.0038, Val Loss: 0.0030\n",
      "Epoch [77/100], Train Loss: 0.0041, Val Loss: 0.0017\n",
      "Epoch [78/100], Train Loss: 0.0033, Val Loss: 0.0014\n",
      "Epoch [79/100], Train Loss: 0.0045, Val Loss: 0.0012\n",
      "Epoch [80/100], Train Loss: 0.0037, Val Loss: 0.0022\n",
      "Epoch [81/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "Epoch [82/100], Train Loss: 0.0039, Val Loss: 0.0006\n",
      "Epoch [83/100], Train Loss: 0.0042, Val Loss: 0.0040\n",
      "Epoch [84/100], Train Loss: 0.0049, Val Loss: 0.0007\n",
      "Epoch [85/100], Train Loss: 0.0038, Val Loss: 0.0012\n",
      "Epoch [86/100], Train Loss: 0.0039, Val Loss: 0.0018\n",
      "Epoch [87/100], Train Loss: 0.0041, Val Loss: 0.0006\n",
      "Epoch [88/100], Train Loss: 0.0041, Val Loss: 0.0006\n",
      "Epoch [89/100], Train Loss: 0.0033, Val Loss: 0.0006\n",
      "Epoch [90/100], Train Loss: 0.0045, Val Loss: 0.0012\n",
      "Epoch [91/100], Train Loss: 0.0037, Val Loss: 0.0009\n",
      "Epoch [92/100], Train Loss: 0.0036, Val Loss: 0.0009\n",
      "Epoch [93/100], Train Loss: 0.0043, Val Loss: 0.0009\n",
      "Epoch [94/100], Train Loss: 0.0051, Val Loss: 0.0006\n",
      "Epoch [95/100], Train Loss: 0.0034, Val Loss: 0.0008\n",
      "Epoch [96/100], Train Loss: 0.0038, Val Loss: 0.0017\n",
      "Epoch [97/100], Train Loss: 0.0037, Val Loss: 0.0008\n",
      "Epoch [98/100], Train Loss: 0.0042, Val Loss: 0.0010\n",
      "Epoch [99/100], Train Loss: 0.0042, Val Loss: 0.0032\n",
      "Epoch [100/100], Train Loss: 0.0031, Val Loss: 0.0021\n",
      "\n",
      "Fold Results:\n",
      "fold: 3.0000\n",
      "mse: 0.0038\n",
      "rmse: 0.0613\n",
      "mae: 0.0529\n",
      "r2: -0.2332\n",
      "mape: 6.0486\n",
      "\n",
      "Fold 4/5\n",
      "Epoch [1/100], Train Loss: 0.1214, Val Loss: 0.0089\n",
      "Epoch [2/100], Train Loss: 0.0325, Val Loss: 0.0027\n",
      "Epoch [3/100], Train Loss: 0.0173, Val Loss: 0.0042\n",
      "Epoch [4/100], Train Loss: 0.0112, Val Loss: 0.0028\n",
      "Epoch [5/100], Train Loss: 0.0112, Val Loss: 0.0067\n",
      "Epoch [6/100], Train Loss: 0.0123, Val Loss: 0.0037\n",
      "Epoch [7/100], Train Loss: 0.0114, Val Loss: 0.0051\n",
      "Epoch [8/100], Train Loss: 0.0110, Val Loss: 0.0021\n",
      "Epoch [9/100], Train Loss: 0.0072, Val Loss: 0.0021\n",
      "Epoch [10/100], Train Loss: 0.0084, Val Loss: 0.0035\n",
      "Epoch [11/100], Train Loss: 0.0090, Val Loss: 0.0019\n",
      "Epoch [12/100], Train Loss: 0.0101, Val Loss: 0.0034\n",
      "Epoch [13/100], Train Loss: 0.0103, Val Loss: 0.0054\n",
      "Epoch [14/100], Train Loss: 0.0112, Val Loss: 0.0018\n",
      "Epoch [15/100], Train Loss: 0.0103, Val Loss: 0.0030\n",
      "Epoch [16/100], Train Loss: 0.0107, Val Loss: 0.0050\n",
      "Epoch [17/100], Train Loss: 0.0127, Val Loss: 0.0021\n",
      "Epoch [18/100], Train Loss: 0.0082, Val Loss: 0.0043\n",
      "Epoch [19/100], Train Loss: 0.0092, Val Loss: 0.0037\n",
      "Epoch [20/100], Train Loss: 0.0081, Val Loss: 0.0018\n",
      "Epoch [21/100], Train Loss: 0.0072, Val Loss: 0.0019\n",
      "Epoch [22/100], Train Loss: 0.0079, Val Loss: 0.0020\n",
      "Epoch [23/100], Train Loss: 0.0092, Val Loss: 0.0017\n",
      "Epoch [24/100], Train Loss: 0.0088, Val Loss: 0.0035\n",
      "Epoch [25/100], Train Loss: 0.0096, Val Loss: 0.0019\n",
      "Epoch [26/100], Train Loss: 0.0083, Val Loss: 0.0018\n",
      "Epoch [27/100], Train Loss: 0.0086, Val Loss: 0.0027\n",
      "Epoch [28/100], Train Loss: 0.0082, Val Loss: 0.0015\n",
      "Epoch [29/100], Train Loss: 0.0089, Val Loss: 0.0017\n",
      "Epoch [30/100], Train Loss: 0.0070, Val Loss: 0.0014\n",
      "Epoch [31/100], Train Loss: 0.0072, Val Loss: 0.0025\n",
      "Epoch [32/100], Train Loss: 0.0076, Val Loss: 0.0014\n",
      "Epoch [33/100], Train Loss: 0.0087, Val Loss: 0.0014\n",
      "Epoch [34/100], Train Loss: 0.0090, Val Loss: 0.0017\n",
      "Epoch [35/100], Train Loss: 0.0082, Val Loss: 0.0013\n",
      "Epoch [36/100], Train Loss: 0.0085, Val Loss: 0.0013\n",
      "Epoch [37/100], Train Loss: 0.0086, Val Loss: 0.0035\n",
      "Epoch [38/100], Train Loss: 0.0078, Val Loss: 0.0024\n",
      "Epoch [39/100], Train Loss: 0.0088, Val Loss: 0.0012\n",
      "Epoch [40/100], Train Loss: 0.0081, Val Loss: 0.0012\n",
      "Epoch [41/100], Train Loss: 0.0075, Val Loss: 0.0013\n",
      "Epoch [42/100], Train Loss: 0.0070, Val Loss: 0.0015\n",
      "Epoch [43/100], Train Loss: 0.0083, Val Loss: 0.0012\n",
      "Epoch [44/100], Train Loss: 0.0078, Val Loss: 0.0013\n",
      "Epoch [45/100], Train Loss: 0.0058, Val Loss: 0.0013\n",
      "Epoch [46/100], Train Loss: 0.0064, Val Loss: 0.0010\n",
      "Epoch [47/100], Train Loss: 0.0057, Val Loss: 0.0015\n",
      "Epoch [48/100], Train Loss: 0.0068, Val Loss: 0.0010\n",
      "Epoch [49/100], Train Loss: 0.0057, Val Loss: 0.0011\n",
      "Epoch [50/100], Train Loss: 0.0069, Val Loss: 0.0010\n",
      "Epoch [51/100], Train Loss: 0.0061, Val Loss: 0.0010\n",
      "Epoch [52/100], Train Loss: 0.0068, Val Loss: 0.0010\n",
      "Epoch [53/100], Train Loss: 0.0060, Val Loss: 0.0016\n",
      "Epoch [54/100], Train Loss: 0.0060, Val Loss: 0.0015\n",
      "Epoch [55/100], Train Loss: 0.0063, Val Loss: 0.0037\n",
      "Epoch [56/100], Train Loss: 0.0061, Val Loss: 0.0015\n",
      "Epoch [57/100], Train Loss: 0.0069, Val Loss: 0.0027\n",
      "Epoch [58/100], Train Loss: 0.0068, Val Loss: 0.0024\n",
      "Epoch [59/100], Train Loss: 0.0061, Val Loss: 0.0011\n",
      "Epoch [60/100], Train Loss: 0.0063, Val Loss: 0.0010\n",
      "Epoch [61/100], Train Loss: 0.0062, Val Loss: 0.0010\n",
      "Epoch [62/100], Train Loss: 0.0072, Val Loss: 0.0019\n",
      "Epoch [63/100], Train Loss: 0.0072, Val Loss: 0.0033\n",
      "Epoch [64/100], Train Loss: 0.0072, Val Loss: 0.0009\n",
      "Epoch [65/100], Train Loss: 0.0053, Val Loss: 0.0009\n",
      "Epoch [66/100], Train Loss: 0.0043, Val Loss: 0.0012\n",
      "Epoch [67/100], Train Loss: 0.0050, Val Loss: 0.0010\n",
      "Epoch [68/100], Train Loss: 0.0053, Val Loss: 0.0012\n",
      "Epoch [69/100], Train Loss: 0.0060, Val Loss: 0.0011\n",
      "Epoch [70/100], Train Loss: 0.0064, Val Loss: 0.0022\n",
      "Epoch [71/100], Train Loss: 0.0059, Val Loss: 0.0008\n",
      "Epoch [72/100], Train Loss: 0.0053, Val Loss: 0.0010\n",
      "Epoch [73/100], Train Loss: 0.0069, Val Loss: 0.0022\n",
      "Epoch [74/100], Train Loss: 0.0063, Val Loss: 0.0008\n",
      "Epoch [75/100], Train Loss: 0.0059, Val Loss: 0.0008\n",
      "Epoch [76/100], Train Loss: 0.0049, Val Loss: 0.0019\n",
      "Epoch [77/100], Train Loss: 0.0054, Val Loss: 0.0008\n",
      "Epoch [78/100], Train Loss: 0.0054, Val Loss: 0.0011\n",
      "Epoch [79/100], Train Loss: 0.0048, Val Loss: 0.0015\n",
      "Epoch [80/100], Train Loss: 0.0053, Val Loss: 0.0010\n",
      "Epoch [81/100], Train Loss: 0.0065, Val Loss: 0.0023\n",
      "Epoch [82/100], Train Loss: 0.0051, Val Loss: 0.0023\n",
      "Epoch [83/100], Train Loss: 0.0050, Val Loss: 0.0011\n",
      "Epoch [84/100], Train Loss: 0.0062, Val Loss: 0.0008\n",
      "Epoch [85/100], Train Loss: 0.0051, Val Loss: 0.0010\n",
      "Epoch [86/100], Train Loss: 0.0048, Val Loss: 0.0008\n",
      "Epoch [87/100], Train Loss: 0.0048, Val Loss: 0.0007\n",
      "Epoch [88/100], Train Loss: 0.0050, Val Loss: 0.0009\n",
      "Epoch [89/100], Train Loss: 0.0041, Val Loss: 0.0007\n",
      "Epoch [90/100], Train Loss: 0.0042, Val Loss: 0.0007\n",
      "Epoch [91/100], Train Loss: 0.0046, Val Loss: 0.0007\n",
      "Epoch [92/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "Epoch [93/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "Epoch [94/100], Train Loss: 0.0048, Val Loss: 0.0008\n",
      "Epoch [95/100], Train Loss: 0.0043, Val Loss: 0.0008\n",
      "Epoch [96/100], Train Loss: 0.0044, Val Loss: 0.0011\n",
      "Epoch [97/100], Train Loss: 0.0046, Val Loss: 0.0009\n",
      "Epoch [98/100], Train Loss: 0.0045, Val Loss: 0.0013\n",
      "Epoch [99/100], Train Loss: 0.0044, Val Loss: 0.0019\n",
      "Epoch [100/100], Train Loss: 0.0044, Val Loss: 0.0007\n",
      "\n",
      "Fold Results:\n",
      "fold: 4.0000\n",
      "mse: 0.0014\n",
      "rmse: 0.0378\n",
      "mae: 0.0290\n",
      "r2: 0.6877\n",
      "mape: 4.2509\n",
      "\n",
      "Fold 5/5\n",
      "Epoch [1/100], Train Loss: 0.1725, Val Loss: 0.0573\n",
      "Epoch [2/100], Train Loss: 0.0479, Val Loss: 0.0052\n",
      "Epoch [3/100], Train Loss: 0.0281, Val Loss: 0.0042\n",
      "Epoch [4/100], Train Loss: 0.0201, Val Loss: 0.0036\n",
      "Epoch [5/100], Train Loss: 0.0176, Val Loss: 0.0033\n",
      "Epoch [6/100], Train Loss: 0.0163, Val Loss: 0.0035\n",
      "Epoch [7/100], Train Loss: 0.0150, Val Loss: 0.0057\n",
      "Epoch [8/100], Train Loss: 0.0152, Val Loss: 0.0040\n",
      "Epoch [9/100], Train Loss: 0.0133, Val Loss: 0.0026\n",
      "Epoch [10/100], Train Loss: 0.0113, Val Loss: 0.0024\n",
      "Epoch [11/100], Train Loss: 0.0113, Val Loss: 0.0024\n",
      "Epoch [12/100], Train Loss: 0.0141, Val Loss: 0.0023\n",
      "Epoch [13/100], Train Loss: 0.0112, Val Loss: 0.0021\n",
      "Epoch [14/100], Train Loss: 0.0130, Val Loss: 0.0021\n",
      "Epoch [15/100], Train Loss: 0.0116, Val Loss: 0.0020\n",
      "Epoch [16/100], Train Loss: 0.0133, Val Loss: 0.0019\n",
      "Epoch [17/100], Train Loss: 0.0116, Val Loss: 0.0023\n",
      "Epoch [18/100], Train Loss: 0.0123, Val Loss: 0.0017\n",
      "Epoch [19/100], Train Loss: 0.0123, Val Loss: 0.0021\n",
      "Epoch [20/100], Train Loss: 0.0138, Val Loss: 0.0018\n",
      "Epoch [21/100], Train Loss: 0.0101, Val Loss: 0.0016\n",
      "Epoch [22/100], Train Loss: 0.0103, Val Loss: 0.0020\n",
      "Epoch [23/100], Train Loss: 0.0104, Val Loss: 0.0015\n",
      "Epoch [24/100], Train Loss: 0.0103, Val Loss: 0.0014\n",
      "Epoch [25/100], Train Loss: 0.0123, Val Loss: 0.0014\n",
      "Epoch [26/100], Train Loss: 0.0119, Val Loss: 0.0013\n",
      "Epoch [27/100], Train Loss: 0.0112, Val Loss: 0.0017\n",
      "Epoch [28/100], Train Loss: 0.0090, Val Loss: 0.0012\n",
      "Epoch [29/100], Train Loss: 0.0121, Val Loss: 0.0010\n",
      "Epoch [30/100], Train Loss: 0.0106, Val Loss: 0.0010\n",
      "Epoch [31/100], Train Loss: 0.0099, Val Loss: 0.0009\n",
      "Epoch [32/100], Train Loss: 0.0124, Val Loss: 0.0023\n",
      "Epoch [33/100], Train Loss: 0.0097, Val Loss: 0.0008\n",
      "Epoch [34/100], Train Loss: 0.0099, Val Loss: 0.0009\n",
      "Epoch [35/100], Train Loss: 0.0107, Val Loss: 0.0009\n",
      "Epoch [36/100], Train Loss: 0.0078, Val Loss: 0.0008\n",
      "Epoch [37/100], Train Loss: 0.0107, Val Loss: 0.0007\n",
      "Epoch [38/100], Train Loss: 0.0124, Val Loss: 0.0008\n",
      "Epoch [39/100], Train Loss: 0.0093, Val Loss: 0.0010\n",
      "Epoch [40/100], Train Loss: 0.0108, Val Loss: 0.0022\n",
      "Epoch [41/100], Train Loss: 0.0083, Val Loss: 0.0012\n",
      "Epoch [42/100], Train Loss: 0.0093, Val Loss: 0.0008\n",
      "Epoch [43/100], Train Loss: 0.0088, Val Loss: 0.0007\n",
      "Epoch [44/100], Train Loss: 0.0098, Val Loss: 0.0022\n",
      "Epoch [45/100], Train Loss: 0.0083, Val Loss: 0.0007\n",
      "Epoch [46/100], Train Loss: 0.0095, Val Loss: 0.0013\n",
      "Epoch [47/100], Train Loss: 0.0086, Val Loss: 0.0017\n",
      "Epoch [48/100], Train Loss: 0.0085, Val Loss: 0.0007\n",
      "Epoch [49/100], Train Loss: 0.0085, Val Loss: 0.0020\n",
      "Epoch [50/100], Train Loss: 0.0085, Val Loss: 0.0013\n",
      "Epoch [51/100], Train Loss: 0.0099, Val Loss: 0.0011\n",
      "Epoch [52/100], Train Loss: 0.0072, Val Loss: 0.0013\n",
      "Epoch [53/100], Train Loss: 0.0079, Val Loss: 0.0006\n",
      "Epoch [54/100], Train Loss: 0.0094, Val Loss: 0.0006\n",
      "Epoch [55/100], Train Loss: 0.0098, Val Loss: 0.0019\n",
      "Epoch [56/100], Train Loss: 0.0084, Val Loss: 0.0014\n",
      "Epoch [57/100], Train Loss: 0.0085, Val Loss: 0.0006\n",
      "Epoch [58/100], Train Loss: 0.0079, Val Loss: 0.0006\n",
      "Epoch [59/100], Train Loss: 0.0077, Val Loss: 0.0005\n",
      "Epoch [60/100], Train Loss: 0.0072, Val Loss: 0.0005\n",
      "Epoch [61/100], Train Loss: 0.0077, Val Loss: 0.0005\n",
      "Epoch [62/100], Train Loss: 0.0069, Val Loss: 0.0005\n",
      "Epoch [63/100], Train Loss: 0.0081, Val Loss: 0.0004\n",
      "Epoch [64/100], Train Loss: 0.0071, Val Loss: 0.0017\n",
      "Epoch [65/100], Train Loss: 0.0081, Val Loss: 0.0016\n",
      "Epoch [66/100], Train Loss: 0.0076, Val Loss: 0.0010\n",
      "Epoch [67/100], Train Loss: 0.0077, Val Loss: 0.0006\n",
      "Epoch [68/100], Train Loss: 0.0072, Val Loss: 0.0005\n",
      "Epoch [69/100], Train Loss: 0.0075, Val Loss: 0.0014\n",
      "Epoch [70/100], Train Loss: 0.0065, Val Loss: 0.0015\n",
      "Epoch [71/100], Train Loss: 0.0075, Val Loss: 0.0006\n",
      "Epoch [72/100], Train Loss: 0.0065, Val Loss: 0.0006\n",
      "Epoch [73/100], Train Loss: 0.0067, Val Loss: 0.0007\n",
      "Epoch [74/100], Train Loss: 0.0063, Val Loss: 0.0005\n",
      "Epoch [75/100], Train Loss: 0.0058, Val Loss: 0.0008\n",
      "Epoch [76/100], Train Loss: 0.0070, Val Loss: 0.0006\n",
      "Epoch [77/100], Train Loss: 0.0066, Val Loss: 0.0008\n",
      "Epoch [78/100], Train Loss: 0.0054, Val Loss: 0.0005\n",
      "Epoch [79/100], Train Loss: 0.0069, Val Loss: 0.0005\n",
      "Epoch [80/100], Train Loss: 0.0058, Val Loss: 0.0005\n",
      "Epoch [81/100], Train Loss: 0.0061, Val Loss: 0.0005\n",
      "Epoch [82/100], Train Loss: 0.0060, Val Loss: 0.0005\n",
      "Epoch [83/100], Train Loss: 0.0053, Val Loss: 0.0008\n",
      "Epoch [84/100], Train Loss: 0.0064, Val Loss: 0.0006\n",
      "Epoch [85/100], Train Loss: 0.0069, Val Loss: 0.0007\n",
      "Epoch [86/100], Train Loss: 0.0064, Val Loss: 0.0009\n",
      "Epoch [87/100], Train Loss: 0.0064, Val Loss: 0.0006\n",
      "Epoch [88/100], Train Loss: 0.0049, Val Loss: 0.0006\n",
      "Epoch [89/100], Train Loss: 0.0054, Val Loss: 0.0007\n",
      "Epoch [90/100], Train Loss: 0.0059, Val Loss: 0.0005\n",
      "Epoch [91/100], Train Loss: 0.0060, Val Loss: 0.0005\n",
      "Epoch [92/100], Train Loss: 0.0055, Val Loss: 0.0005\n",
      "Epoch [93/100], Train Loss: 0.0051, Val Loss: 0.0007\n",
      "Epoch [94/100], Train Loss: 0.0048, Val Loss: 0.0006\n",
      "Epoch [95/100], Train Loss: 0.0052, Val Loss: 0.0010\n",
      "Epoch [96/100], Train Loss: 0.0056, Val Loss: 0.0019\n",
      "Epoch [97/100], Train Loss: 0.0053, Val Loss: 0.0007\n",
      "Epoch [98/100], Train Loss: 0.0053, Val Loss: 0.0006\n",
      "Epoch [99/100], Train Loss: 0.0048, Val Loss: 0.0006\n",
      "Epoch [100/100], Train Loss: 0.0045, Val Loss: 0.0005\n",
      "\n",
      "Fold Results:\n",
      "fold: 5.0000\n",
      "mse: 0.0010\n",
      "rmse: 0.0312\n",
      "mae: 0.0241\n",
      "r2: 0.9220\n",
      "mape: 4.8456\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Average MSE: 0.0040\n",
      "Average RMSE: 0.0579\n",
      "Average MAE: 0.0464\n",
      "Average R2: 0.4657\n",
      "Average MAPE: 6.9641\n"
     ]
    }
   ],
   "source": [
    "features = ['Adj Close', 'Volume', 'Price_Change', 'Volume_Change', 'MA_5', 'MA_10', 'MA_20', 'MA_50', 'sentiment']\n",
    "model, scaler, fold_metrics, avg_metrics = prepare_and_train_model(df, features, sequence_length=24, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254df69-d4bb-4012-8af3-934d90baa405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
